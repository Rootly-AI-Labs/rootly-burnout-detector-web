# Claude Development Session Notes

## Project: OnCall Burnout Detector - Railway Deployment & Analysis Fixes

### Recent Major Work Completed

#### 1. UUID Implementation for Unique Analysis URLs ‚úÖ
- **Issue**: Analyses used predictable integer IDs, not shareable
- **Solution**: Added UUID field to Analysis model with automatic generation
- **Files Changed**:
  - `backend/app/models/analysis.py` - Added UUID column
  - `backend/app/api/endpoints/analyses.py` - Added UUID endpoints & responses
  - `frontend/src/app/dashboard/page.tsx` - Updated to use UUIDs in URLs
- **Migration**: Created PostgreSQL migration scripts for Railway
- **Status**: Completed - analyses now have unique shareable URLs

#### 2. Railway PostgreSQL Compatibility Fixes ‚úÖ
- **Issue**: SQLAlchemy errors on Railway due to missing UUID column
- **Solution**: Graceful handling of missing columns during migration
- **Files Changed**:
  - `backend/migrate_uuid_step1.py` - Database migration script
  - `backend/app/api/endpoints/analyses.py` - Safe UUID access with getattr()
  - `frontend/src/app/dashboard/page.tsx` - Fallback to integer IDs
- **Status**: Completed - app works before and after migration

#### 3. Analysis Execution Error Fixes ‚úÖ
- **Issue**: Multiple "NoneType to numerator/denominator" errors
- **Solution**: Added None checks to all mathematical operations
- **Files Changed**:
  - `backend/app/services/burnout_analyzer.py` - Fixed division operations
  - `backend/app/core/burnout_analyzer.py` - Added None checks
  - `backend/app/services/unified_burnout_analyzer.py` - Protected calculations
  - `backend/app/agents/tools/*.py` - Added missing logger imports
- **Status**: Completed - analyses run without NoneType errors

#### 4. Burnout Factors Chart Data Fix ‚úÖ
- **Issue**: Chart showed all zeros despite hundreds of incidents
- **Solution**: Added missing 'factors' structure to SimpleBurnoutAnalyzer
- **Files Changed**:
  - `backend/app/core/simple_burnout_analyzer.py` - Added factors calculation
- **Status**: Completed - burnout factors now show actual values

#### 5. Historical Analyses Visibility Fix ‚úÖ
- **Issue**: Old analyses not showing due to UUID column errors
- **Solution**: Safe UUID field access with proper fallbacks
- **Files Changed**:
  - `backend/app/api/endpoints/analyses.py` - getattr() for safe access
- **Status**: Completed - all historical analyses visible

### CRITICAL ISSUE FIXED - Health Trends Chart Logic ‚úÖ

#### Issue Resolution:
The health trends chart was showing historical analysis results instead of daily incident data from the current analysis period.

#### Changes Made:
1. **Backend**: Modified `/analyses/trends/historical` endpoint in `backend/app/api/endpoints/analyses.py`
   - Now returns daily incident trends from the most recent analysis
   - Uses the `daily_trends` data already generated by the burnout analyzer
   - Each data point shows daily incident count, health score, and members at risk
   - Filters by `days_back` parameter for chart display range

2. **Data Structure**: The endpoint now returns:
   - Date for each day with incident data
   - Daily incident count and health scores
   - Members at risk based on incident patterns
   - Health status (critical, at_risk, moderate, healthy)
   - Timeline events for significant health changes

#### Technical Details:
- Uses most recent completed analysis instead of aggregating historical analyses
- Extracts `daily_trends` from analysis results (generated by `_generate_daily_trends`)
- Maintains same API response format for frontend compatibility
- Filters trends by date range specified in `days_back` parameter

### Testing Commands:
```bash
# Run backend
cd backend && python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Check database migrations needed on Railway
cd backend && python run_uuid_migration.py

# Debug trends data
cd backend && python debug_trends_data.py
```

### Known Working Features:
- ‚úÖ Analysis creation and execution
- ‚úÖ Burnout factors chart with real data
- ‚úÖ Historical analyses list
- ‚úÖ UUID-based shareable URLs
- ‚úÖ Background analysis processing
- ‚úÖ Error handling and recovery

### CRITICAL ISSUES TO FIX - EXECUTION PLAN

#### Issue #1: Frontend Shows Fallback/Demo Data for Non-Existent Analyses
**Problem**: URL shows `?analysis=64` but analysis doesn't exist, causing frontend to show hardcoded demo data (110 incidents, 18 days)
**Impact**: Users see inconsistent fake data that doesn't match any real analysis

**Fix Plan**:
1. **Frontend**: Add proper error handling when analysis not found
   - File: `frontend/src/app/dashboard/page.tsx`
   - Check if analysis exists before displaying data
   - Show clear error message: "Analysis not found" 
   - Redirect to most recent valid analysis or show empty state
   - Remove hardcoded fallback data (110 incidents, 18 days pattern)
   - **CRITICAL: NO FALLBACK DATA - Always show real state:**
     - If no data: "No incident data available"
     - If 0 incidents: "0 incidents analyzed" 
     - If API error: "Failed to fetch incidents: [error message]"
     - If daily trends empty: "No daily trend data generated"
     - NEVER show fake/demo data

2. **Backend**: Return proper 404 with redirect suggestion
   - File: `backend/app/api/endpoints/analyses.py`
   - When analysis not found, include most recent valid analysis ID in error response
   - Frontend can use this to auto-redirect
   - Include error details in response for frontend to display

#### Issue #2: Daily Trends Generation Completely Broken
**Problem**: ALL 78 completed analyses have 0 daily trends data, causing empty charts
**Impact**: Health trends chart shows no real data for any analysis

**Fix Plan**:
1. **Immediate Fix**: Add data regeneration endpoint
   - Create `/analyses/{id}/regenerate-trends` endpoint
   - Regenerate daily trends for existing analyses
   - Use the incident generation logic when API permissions blocked

2. **Root Cause Fix**: Ensure daily trends always generated
   - File: `backend/app/services/burnout_analyzer.py`
   - Add validation that daily_trends is never empty
   - If no incidents but time period exists, generate empty daily entries
   - Log warnings when daily trends generation fails

#### Issue #3: UUID Implementation Incomplete
**Problem**: UUID column commented out, using integer IDs, no shareable URLs
**Impact**: URLs not shareable, sequential IDs expose data

**Fix Plan**:
1. **Complete UUID Implementation**:
   - File: `backend/app/models/analysis.py`
   - Uncomment UUID column: `uuid = Column(String(36), unique=True, index=True, nullable=True, default=lambda: str(uuid.uuid4()))`
   - Commit and push the change

2. **Run Migration on Railway**:
   - The migration script already exists: `backend/migrate_uuid_step1.py`
   - Run via Railway CLI or web console
   - Verify UUID column added to all existing analyses

3. **Update Frontend to Use UUIDs**:
   - File: `frontend/src/app/dashboard/page.tsx`
   - Switch from `?analysis={id}` to `?analysis={uuid}`
   - Keep fallback to integer ID for backward compatibility

#### Issue #4: API Permissions Preventing Real Data
**Problem**: Rootly API returns 404 on incidents endpoint (missing incidents:read permission)
**Impact**: All analyses show 0 real incidents, only generated/fake data

**Fix Plan**:
1. **Update Rootly API Token**:
   - Get new token with 'incidents:read' permission from Rootly
   - Update in integration settings
   
2. **Add Permission Check on Integration Setup**:
   - File: `backend/app/api/endpoints/rootly.py`
   - Test permissions when saving integration
   - Show clear warning if incidents:read missing

#### Issue #5: Data Consistency Between Components
**Problem**: Different dashboard components show different data sources
**Impact**: Total incidents, charts, and metrics don't match

**Fix Plan**:
1. **Single Source of Truth**:
   - All components must read from analysis.results
   - Remove any hardcoded values
   - Add data validation to ensure consistency
   - **NO FALLBACK DATA PRINCIPLE**:
     - Every component shows EXACTLY what's in the database
     - If data is missing, show "Data not available" 
     - If calculation fails, show "Unable to calculate"
     - Log all issues for debugging

2. **Add Data Integrity Check**:
   - Create consistency validator
   - Run before displaying analysis
   - Log any discrepancies found
   - Display warnings to user when data inconsistencies detected

#### CORE PRINCIPLE: Transparency Over Prettiness
**Always show the real state of the system:**
- Empty data ‚Üí Show empty state with explanation
- Failed API calls ‚Üí Show error with details
- Missing permissions ‚Üí Show "Insufficient permissions to access incidents"
- Broken calculations ‚Üí Show "Calculation error" with details
- NEVER hide problems with fake data

## COMPREHENSIVE ERROR HANDLING SYSTEM - IMPLEMENTATION PLAN

### Overview
Create a robust error display system for the analytics dashboard that provides users with clear, actionable information about any issues affecting their data analysis.

### 1. Error Classification System

#### **Critical System Errors** (üö® Red)
- **Analysis Failures**: Burnout analysis crashes or timeouts
- **Database Connection Issues**: PostgreSQL connection failures
- **Authentication Failures**: Invalid or expired tokens
- **API Unavailability**: Backend services down or unresponsive

#### **Data Collection Errors** (‚ö†Ô∏è Orange) 
- **API Permission Issues**: Missing incidents:read, users:read permissions
- **Rate Limiting**: GitHub/Rootly/Slack API rate limits exceeded
- **Token Issues**: Expired or invalid integration tokens
- **Network Timeouts**: API calls timing out during data collection

#### **Data Quality Warnings** (‚ÑπÔ∏è Blue)
- **Incomplete Data**: Missing incidents, users, or activity data
- **No Matches Found**: GitHub email-to-username correlation failures
- **Stale Data**: Data older than expected refresh intervals
- **Configuration Issues**: Missing integrations or incomplete setup

#### **Processing Warnings** (üí° Yellow)
- **Fallback Data Used**: Using generated/approximated data when real data unavailable
- **Performance Concerns**: Analysis taking longer than expected
- **Feature Limitations**: AI features unavailable due to missing LLM tokens
- **Mapping Issues**: User-to-platform correlation problems

### 2. Frontend Error Display Components

#### **Error Icon & Badge (Top Right)**
```tsx
// Location: Top right of dashboard header
<div className="relative">
  <Button 
    variant="ghost" 
    size="sm"
    onClick={() => setErrorModalOpen(true)}
    className={cn(
      "relative",
      hasErrors && "text-red-500 hover:text-red-600",
      hasWarnings && !hasErrors && "text-orange-500 hover:text-orange-600"
    )}
  >
    <AlertTriangle className="h-4 w-4" />
    {(errorCount + warningCount) > 0 && (
      <Badge 
        variant={hasErrors ? "destructive" : "secondary"}
        className="absolute -top-2 -right-2 px-1 min-w-[16px] h-4 text-xs"
      >
        {errorCount + warningCount}
      </Badge>
    )}
  </Button>
</div>
```

#### **Error Modal Component**
```tsx
interface ErrorModalProps {
  open: boolean
  onOpenChange: (open: boolean) => void
  errors: SystemError[]
  warnings: SystemWarning[]
}

interface SystemError {
  id: string
  type: 'critical' | 'data_collection' | 'data_quality' | 'processing'
  title: string
  message: string
  details?: string
  timestamp: string
  component: string  // Which part of the system
  suggested_action?: string
  documentation_link?: string
  can_retry?: boolean
  can_dismiss?: boolean
}
```

#### **Error Display Categories**
1. **Current Session Errors**: Issues from the active analysis
2. **Recent Errors**: Last 24 hours of issues
3. **Persistent Issues**: Ongoing problems affecting data quality
4. **System Status**: Overall health indicators

### 3. Backend Error Tracking & Logging

#### **Error Collection Service**
```python
# File: backend/app/services/error_tracking_service.py
class ErrorTrackingService:
    """Centralized error tracking for user-facing issues."""
    
    def __init__(self):
        self.user_errors = {}  # user_id -> [errors]
    
    def log_user_error(
        self, 
        user_id: int, 
        error_type: str,
        component: str,
        title: str,
        message: str,
        details: Optional[str] = None,
        suggested_action: Optional[str] = None,
        can_retry: bool = False
    ):
        """Log an error that should be shown to the user."""
        
    def get_user_errors(self, user_id: int) -> List[Dict]:
        """Get all errors for a specific user."""
        
    def clear_user_errors(self, user_id: int, error_ids: List[str] = None):
        """Clear specific or all errors for a user."""
```

#### **Error Integration Points**

**Rootly API Client Errors**:
```python
# In: backend/app/core/rootly_client.py
try:
    response = await self.client.get("/incidents")
except httpx.HTTPStatusError as e:
    if e.response.status_code == 404:
        error_service.log_user_error(
            user_id=current_user.id,
            error_type="data_collection",
            component="rootly_api",
            title="Incidents API Access Denied",
            message="Unable to fetch incident data. Missing 'incidents:read' permission.",
            suggested_action="Update your Rootly API token with incidents:read permission",
            can_retry=True
        )
```

**GitHub Token Validation**:
```python
# In: backend/app/services/github_collector.py
except aiohttp.ClientError as e:
    if "401" in str(e):
        error_service.log_user_error(
            user_id=current_user.id,
            error_type="data_collection", 
            component="github_api",
            title="GitHub Token Expired",
            message="Your GitHub token has expired or is invalid.",
            suggested_action="Update your GitHub token in Settings > Integrations",
            can_retry=True
        )
```

**Analysis Processing Errors**:
```python
# In: backend/app/services/unified_burnout_analyzer.py
except Exception as e:
    error_service.log_user_error(
        user_id=current_user.id,
        error_type="critical",
        component="burnout_analysis",
        title="Analysis Failed",
        message=f"Burnout analysis could not be completed: {str(e)}",
        details=traceback.format_exc(),
        suggested_action="Try running the analysis again, or contact support if the issue persists",
        can_retry=True
    )
```

### 4. API Endpoints for Error Management

#### **Get User Errors**
```python
@router.get("/errors")
async def get_user_errors(
    current_user: User = Depends(get_current_user)
):
    """Get all errors and warnings for the current user."""
    return {
        "errors": error_service.get_user_errors(current_user.id),
        "warnings": warning_service.get_user_warnings(current_user.id),
        "system_status": {
            "database": "healthy",
            "apis": {
                "rootly": "healthy",
                "github": "degraded",  # If rate limited
                "slack": "healthy"
            }
        }
    }

@router.post("/errors/{error_id}/dismiss")
async def dismiss_error(
    error_id: str,
    current_user: User = Depends(get_current_user)
):
    """Dismiss a specific error."""
    
@router.post("/errors/{error_id}/retry")
async def retry_failed_operation(
    error_id: str,
    current_user: User = Depends(get_current_user)
):
    """Retry a failed operation."""
```

### 5. Error Message Templates

#### **Common Error Messages**
```typescript
const ERROR_MESSAGES = {
  // API Permission Issues
  ROOTLY_INCIDENTS_PERMISSION: {
    title: "Missing Incidents Permission", 
    message: "Your Rootly API token doesn't have permission to read incidents.",
    action: "Update your token with 'incidents:read' permission"
  },
  
  // Token Issues  
  GITHUB_TOKEN_EXPIRED: {
    title: "GitHub Token Expired",
    message: "Your GitHub personal access token has expired.",
    action: "Generate a new token and update it in Settings"
  },
  
  // Data Quality Issues
  NO_GITHUB_MATCHES: {
    title: "No GitHub Activity Found", 
    message: "Unable to match team members to GitHub accounts.",
    action: "Review GitHub mappings or add manual mappings"
  },
  
  // Analysis Issues
  ANALYSIS_TIMEOUT: {
    title: "Analysis Timed Out",
    message: "The burnout analysis took too long to complete.",
    action: "Try reducing the time range or contact support"
  }
}
```

### 6. Implementation Steps

#### **Phase 1: Backend Error Tracking** (2-3 hours)
1. Create `ErrorTrackingService` and `WarningService`
2. Add error logging to key failure points:
   - API token validation
   - Data collection (Rootly, GitHub, Slack)
   - Analysis processing
   - Database operations
3. Create `/api/errors` endpoints

#### **Phase 2: Frontend Error Display** (3-4 hours)
1. Create `ErrorModal` component with categorized display
2. Add error icon/badge to dashboard header
3. Implement error state management (Context or Zustand)
4. Add error polling/real-time updates

#### **Phase 3: Error Actions & Recovery** (2-3 hours)
1. Implement retry mechanisms for failed operations
2. Add "Quick Fix" actions (redirect to token settings, etc.)
3. Add error dismissal and persistence
4. Create error analytics/reporting

#### **Phase 4: Enhanced Error Details** (1-2 hours)
1. Add documentation links for common issues
2. Implement error search and filtering
3. Add error export/sharing capabilities
4. Create error resolution tracking

### 7. File Structure

```
frontend/src/
‚îú‚îÄ‚îÄ components/errors/
‚îÇ   ‚îú‚îÄ‚îÄ ErrorModal.tsx           # Main error display modal
‚îÇ   ‚îú‚îÄ‚îÄ ErrorBadge.tsx          # Top-right error indicator
‚îÇ   ‚îú‚îÄ‚îÄ ErrorCard.tsx           # Individual error display
‚îÇ   ‚îî‚îÄ‚îÄ ErrorActions.tsx        # Retry/dismiss actions
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îú‚îÄ‚îÄ useErrorTracking.ts     # Error state management
‚îÇ   ‚îî‚îÄ‚îÄ useSystemHealth.ts      # System status monitoring
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ errors.ts               # Error type definitions

backend/app/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ error_tracking_service.py    # Centralized error tracking
‚îÇ   ‚îî‚îÄ‚îÄ warning_service.py           # Warning management
‚îú‚îÄ‚îÄ api/endpoints/
‚îÇ   ‚îî‚îÄ‚îÄ errors.py                    # Error management endpoints
‚îî‚îÄ‚îÄ middleware/
    ‚îî‚îÄ‚îÄ error_interceptor.py         # Automatic error capture
```

### 8. User Experience Features

#### **Error Categorization**
- **üö® Blocking Issues**: Prevent analysis from running
- **‚ö†Ô∏è Data Issues**: Analysis runs but data may be incomplete  
- **‚ÑπÔ∏è Informational**: System working but user should be aware
- **üí° Suggestions**: Optimization or improvement opportunities

#### **Quick Actions**
- **"Fix Now"**: Direct links to settings/integration pages
- **"Retry Analysis"**: Re-run failed operations
- **"Contact Support"**: Pre-filled support request with error details
- **"Learn More"**: Links to documentation

#### **Error Persistence**
- Errors persist across page refreshes
- Automatic dismissal after successful retry
- Manual dismissal for non-critical issues
- Error history for troubleshooting patterns

### 9. Success Metrics

#### **User Experience Metrics**
- **Error Resolution Rate**: % of errors that users successfully fix
- **Time to Resolution**: How quickly users resolve issues
- **Support Ticket Reduction**: Fewer support requests due to better error messaging
- **Feature Adoption**: More users successfully setting up integrations

#### **System Health Metrics**  
- **Error Frequency**: Trending of different error types
- **Component Reliability**: Which parts of the system fail most often
- **User Impact**: How many users affected by each error type
- **Recovery Success**: Success rate of retry operations

This comprehensive error handling system will transform user experience from "something's broken" to "here's exactly what's wrong and how to fix it."

## COMPREHENSIVE FALLBACK DATA AUDIT & REMOVAL PLAN

### CORE PRINCIPLE: RADICAL TRANSPARENCY - NO FAKE DATA
**If we don't have real data, we show "No data available" - NEVER fake it**

### Overview
Conduct a systematic audit of the entire analysis dashboard to identify and eliminate ALL fallback, mock, placeholder, and generated data. Users should see the exact state of their system, not idealized versions.

### Audit Areas & Methodology

#### **1. FRONTEND DASHBOARD COMPONENTS**
**File**: `frontend/src/app/dashboard/page.tsx`

##### **1.1 Top Statistics Cards**
- [ ] **Team Health Card**: Check for hardcoded health scores when no analysis available
- [ ] **At Risk Members Card**: Verify count comes from real analysis, not estimates
- [ ] **Total Incidents Card**: Ensure incident count is from actual API data
- [ ] **Active Period Card**: Confirm days analyzed matches real data collection period

##### **1.2 Charts & Visualizations**
- [ ] **Health Trends Chart**: Remove any generated trend data, sample points, or interpolated values
- [ ] **Burnout Factors Chart**: Eliminate placeholder factor scores when no incidents
- [ ] **Team Overview Chart**: Remove mock member data or estimated scores
- [ ] **Any other charts**: Verify all data points represent real measurements

##### **1.3 Activity Cards**
- [ ] **GitHub Activity Card**: Remove placeholder commits, PRs, review counts when no real data
- [ ] **Slack Activity Card**: Eliminate sample message counts, sentiment scores without real data
- [ ] **Integration Status**: Show actual connection status, not assumed/default states

##### **1.4 Team Members Section**
- [ ] **Member Cards**: Remove generated burnout scores, risk levels without incident data
- [ ] **Member Avatars**: Don't show placeholder/generic avatars for unmapped users
- [ ] **Activity Indicators**: Remove fake GitHub/Slack activity badges
- [ ] **Burnout Scores**: Eliminate calculated scores when insufficient data

##### **1.5 Insights & Recommendations**
- [ ] **AI Insights**: Remove generated text when no LLM token or real analysis
- [ ] **Recommendations**: Eliminate template recommendations not based on actual patterns
- [ ] **Pattern Detection**: Remove pattern alerts without real data backing

#### **2. BACKEND ANALYSIS SERVICES**
**Files**: All analyzer services and data collectors

##### **2.1 UnifiedBurnoutAnalyzer**
**File**: `backend/app/services/unified_burnout_analyzer.py`
- [ ] **Member Analysis**: Check `_analyze_member_burnout()` for fallback scoring
- [ ] **Team Health Calculation**: Review `_calculate_team_health()` for default values
- [ ] **Daily Trends**: Audit `_generate_daily_trends()` for interpolated/generated days
- [ ] **Burnout Factors**: Verify `_calculate_burnout_factors()` doesn't use defaults
- [ ] **Recommendations**: Check `_generate_recommendations()` for template responses

##### **2.2 SimpleBurnoutAnalyzer**
**File**: `backend/app/core/simple_burnout_analyzer.py`
- [ ] **Period Summary**: Check for hardcoded average scores when no data
- [ ] **Risk Assessment**: Review risk level calculations for default assignments
- [ ] **Team Summary**: Audit aggregation functions for fallback values
- [ ] **Health Scoring**: Verify no baseline health assumptions

##### **2.3 GitHub Data Collection**
**File**: `backend/app/services/github_collector.py`
- [ ] **Mock Data Generation**: Remove `_generate_mock_github_data()` entirely
- [ ] **Activity Defaults**: Check for placeholder commit/PR counts
- [ ] **Burnout Indicators**: Verify indicators based on real metrics only
- [ ] **Failed Collection**: Return None instead of mock data on API failures

##### **2.4 Slack Data Collection**
**File**: `backend/app/services/slack_collector.py` (if exists)
- [ ] **Message Counts**: Remove default/estimated message counts
- [ ] **Sentiment Analysis**: Don't show sentiment without real message data
- [ ] **Channel Activity**: Remove placeholder channel participation data
- [ ] **Communication Patterns**: Eliminate pattern detection without real data

##### **2.5 AI Analysis Services**
**File**: `backend/app/services/ai_burnout_analyzer.py`
- [ ] **Fallback Analysis**: Remove `_fallback_analysis()` method or make it return "unavailable"
- [ ] **Generated Insights**: Don't create insights without real LLM processing
- [ ] **Template Responses**: Remove any canned AI-like responses
- [ ] **Pattern Recognition**: Return empty results when no real analysis possible

#### **3. API RESPONSE DATA**
**Files**: All API endpoint files

##### **3.1 Analysis Endpoints**
**File**: `backend/app/api/endpoints/analyses.py`
- [ ] **Analysis Results**: Check for fallback analysis data in responses
- [ ] **Metadata**: Verify all metadata reflects actual data collection
- [ ] **Team Health**: Ensure health scores based on real calculations
- [ ] **Daily Trends**: Confirm trends represent actual incident patterns

##### **3.2 Integration Endpoints**
- [ ] **GitHub Status**: Show real connection status, not assumed connectivity
- [ ] **Slack Status**: Return actual integration state, not defaults
- [ ] **Token Validation**: Report real validation results, not optimistic assumptions

#### **4. DATABASE DATA INTEGRITY**
**Files**: Database models and data storage

##### **4.1 Analysis Results Storage**
- [ ] **Stored Analysis**: Check for placeholder data in database records
- [ ] **Team Analysis**: Verify member data represents real measurements
- [ ] **Factors Data**: Ensure burnout factors calculated from actual incidents
- [ ] **Metadata Accuracy**: Confirm stored metadata matches real collection scope

#### **5. SPECIFIC PATTERNS TO ELIMINATE**

##### **5.1 Frontend Patterns**
```typescript
// REMOVE patterns like:
const fallbackData = { /* mock data */ }
const data = realData || fallbackData  // ‚ùå BAD

// REPLACE with:
const data = realData || null
if (!data) {
  return <EmptyState message="No data available" />
}
```

##### **5.2 Backend Patterns**
```python
# REMOVE patterns like:
if not real_data:
    return generate_mock_data()  # ‚ùå BAD

# REPLACE with:
if not real_data:
    return None  # ‚úÖ GOOD
```

##### **5.3 Chart Data Patterns**
```typescript
// REMOVE patterns like:
const chartData = realData.length > 0 ? realData : sampleData  // ‚ùå BAD

// REPLACE with:
const chartData = realData
if (chartData.length === 0) {
  return <EmptyChart message="No data to display" />
}
```

### **AUDIT EXECUTION PLAN**

#### **Phase 1: Frontend Audit** (Comprehensive Review)
1. **Dashboard Page Analysis** (`frontend/src/app/dashboard/page.tsx`)
   - Line-by-line review of all data usage
   - Identify every `||` fallback operator
   - Check all default values and placeholder data
   - Verify chart data sources

2. **Component Analysis**
   - Individual component audits for fallback patterns
   - Props validation for required vs optional data
   - State management review for default values

#### **Phase 2: Backend Service Audit** (Deep Dive)
1. **Analyzer Services**
   - Method-by-method review of calculation logic  
   - Identify any hardcoded defaults or baselines
   - Check error handling for missing data scenarios
   - Verify return values when no data available

2. **Data Collectors**
   - API failure handling review
   - Mock data generation removal
   - Validation of empty result handling

#### **Phase 3: API Response Audit** (Data Integrity)
1. **Endpoint Response Structure**
   - Verify response schemas don't include fallback fields
   - Check for consistent null/empty handling
   - Validate metadata accuracy

#### **Phase 4: Database Audit** (Stored Data Verification)
1. **Analysis Records**
   - Check for placeholder data in stored analyses
   - Verify data consistency across analysis runs
   - Confirm metadata matches actual collection

### **REPLACEMENT PATTERNS**

#### **Instead of Fallback Data, Show:**

##### **No Data States**
```typescript
// For cards with no data
<Card>
  <CardHeader>
    <CardTitle>GitHub Activity</CardTitle>
  </CardHeader>
  <CardContent>
    <div className="text-center py-8 text-muted-foreground">
      <Database className="h-12 w-12 mx-auto mb-2 opacity-50" />
      <p>No GitHub data available</p>
      <p className="text-sm">Connect GitHub integration to see activity</p>
    </div>
  </CardContent>
</Card>
```

##### **Empty Charts**
```typescript
// For charts with no data
<div className="text-center py-12 text-muted-foreground">
  <BarChart3 className="h-16 w-16 mx-auto mb-4 opacity-50" />
  <h3 className="text-lg font-semibold mb-2">No trend data available</h3>
  <p>Run an analysis with incident data to see health trends</p>
</div>
```

##### **Missing Integration States**
```typescript
// For disconnected integrations
<Alert>
  <AlertCircle className="h-4 w-4" />
  <AlertTitle>GitHub Integration Required</AlertTitle>
  <AlertDescription>
    Connect your GitHub account to see development activity and burnout indicators.
    <Button variant="link" className="p-0 h-auto">
      Connect GitHub ‚Üí
    </Button>
  </AlertDescription>
</Alert>
```

### **VALIDATION CHECKLIST**

#### **After Each Audit Section:**
- [ ] **No Fallback Data**: Confirm zero fallback/mock/placeholder data remains
- [ ] **Accurate Empty States**: Verify appropriate empty state messaging
- [ ] **Real Data Only**: Ensure all displayed values represent actual measurements  
- [ ] **Consistent Messaging**: Check for consistent "no data" language
- [ ] **User Guidance**: Confirm users understand how to get real data

#### **Final Integration Test:**
- [ ] **Fresh Database**: Test with completely empty database
- [ ] **No Integrations**: Test with no GitHub/Slack connections
- [ ] **No Incidents**: Test with zero incident data
- [ ] **Partial Data**: Test with only some integrations connected
- [ ] **API Failures**: Test with API endpoints returning errors

### **DOCUMENTATION REQUIREMENTS**

#### **For Each Removed Fallback:**
- **Location**: File and line number
- **What Was Removed**: Description of fallback data
- **Replacement**: How empty state is now handled
- **User Impact**: What users will see instead

#### **Summary Report Format:**
```
FALLBACK REMOVAL SUMMARY
========================
Component: GitHub Activity Card
File: frontend/src/app/dashboard/page.tsx:450-480
Removed: Mock commit counts (25 commits, 5 PRs)
Replaced: "No GitHub data available" empty state
User Impact: Users see honest "no data" instead of fake activity

[Continue for each removal...]
```

This systematic audit will ensure users see exactly what data exists in their system - no more, no less. The goal is radical transparency about data availability.

### IMMEDIATE ACTIONS - Remove All Fallback Data

**Search and Remove These Patterns**:
1. **Frontend** (`frontend/src/app/dashboard/page.tsx`):
   - Any hardcoded "110 incidents"
   - Any "18 days" pattern
   - Default health scores when data missing
   - Mock timeline events
   - Placeholder member data
   
2. **Backend** (all analyzer files):
   - Remove any demo/mock data generation
   - Remove default values that hide real issues
   - Keep ONLY the incident generation for consistency (when metadata shows incidents but API fails)

**Replace With**:
```typescript
// Instead of: incidents = fallbackData || []
// Use: incidents = data?.incidents || []
// Display: {incidents.length === 0 ? "No incidents to display" : <IncidentChart />}
```

### DATA VERIFICATION & CROSS-CHECK PLAN

#### Automated Data Consistency Verification System

**1. Backend Validation Endpoint** (`/analyses/{id}/verify-consistency`):
```python
# Returns detailed consistency report
{
  "analysis_id": 140,
  "consistency_checks": {
    "incident_totals": {
      "metadata_total": 110,
      "team_analysis_sum": 108,
      "daily_trends_sum": 18,
      "match": false,
      "discrepancy": "Daily trends only sum to 18, not 110"
    },
    "member_counts": {
      "metadata_users": 38,
      "team_analysis_members": 38,
      "members_with_incidents": 5,
      "match": true
    },
    "date_ranges": {
      "metadata_days": 30,
      "daily_trends_days": 18,
      "expected_data_points": 30,
      "actual_data_points": 18,
      "match": false,
      "discrepancy": "Missing 12 days of daily trend data"
    },
    "severity_distribution": {
      "metadata_breakdown": {"SEV1": 3, "SEV2": 60, "SEV3": 16, "SEV4": 31},
      "calculated_total": 110,
      "incidents_with_severity": 110,
      "match": true
    }
  },
  "overall_consistency": false,
  "critical_issues": [
    "Daily trends incomplete: 18/30 days",
    "Daily incident sum (18) doesn't match total (110)"
  ]
}
```

**2. Frontend Consistency Dashboard** (Development Mode):
- Add debug panel showing all data sources
- Visual indicators when numbers don't match
- Color coding: ‚úÖ Green (match), ‚ùå Red (mismatch), ‚ö†Ô∏è Yellow (close but off)

**3. Cross-Component Validation Rules**:

**Top Cards vs Source Data**:
```typescript
// Team Health Card
- Value shown: analysis.results.team_health.overall_score
- Validation: Must be between 0-100
- Cross-check: Average of member scores should be within 10% of overall

// At Risk Card  
- Value shown: analysis.results.team_health.members_at_risk
- Validation: Cannot exceed total members
- Cross-check: Count of members with risk_level !== 'low'

// Total Incidents Card
- Value shown: analysis.results.metadata.total_incidents
- Cross-checks:
  1. Sum of daily_trends[].incident_count
  2. Sum of team_analysis.members[].incident_count
  3. Sum of severity_breakdown values
  - ALL must match exactly
```

**Health Trends Chart vs Source Data**:
```typescript
// Each data point
- Date: daily_trends[i].date
- Score: daily_trends[i].overall_score
- Incidents: daily_trends[i].incident_count

// Validations:
- Number of points === metadata.days_analyzed
- Date range covers full analysis period
- Sum of incidents === metadata.total_incidents
- No duplicate dates
- Dates in chronological order
```

**4. Automated Test Suite**:
```python
# tests/test_data_consistency.py
def test_analysis_data_consistency():
    for analysis in get_all_completed_analyses():
        # Test 1: Incident totals match across all sources
        assert sum(day['incident_count'] for day in analysis.daily_trends) == analysis.metadata.total_incidents
        
        # Test 2: Daily trends cover full time period
        assert len(analysis.daily_trends) == analysis.metadata.days_analyzed
        
        # Test 3: Member incident counts sum to total
        member_incident_sum = sum(m['incident_count'] for m in analysis.team_analysis.members)
        assert member_incident_sum == analysis.metadata.total_incidents
        
        # Test 4: At-risk count matches actual risk levels
        at_risk_calculated = len([m for m in analysis.team_analysis.members if m['risk_level'] != 'low'])
        assert at_risk_calculated == analysis.team_health.members_at_risk
```

**5. Manual Verification Checklist** (For QA):
- [ ] Total Incidents card matches sum of severity breakdown
- [ ] Health trends chart shows correct number of days
- [ ] Each day's incidents sum to total when added
- [ ] Team health percentage matches calculation
- [ ] At-risk count matches highlighted members
- [ ] Timeline events correspond to daily trend data
- [ ] Burnout factors chart values sum correctly
- [ ] All dates match analysis time range

**6. Real-time Consistency Monitor**:
- Add console warnings when displaying inconsistent data
- Log discrepancies to monitoring system
- Email alert if consistency < 95% for new analyses

## COMPREHENSIVE FALLBACK DATA AUDIT RESULTS

### Critical Fallback Patterns Found in `frontend/src/app/dashboard/page.tsx`

#### 1. GitHub Activity Card Fallbacks (Lines 3900-3920)
**CRITICAL ISSUES**:
- `github.total_commits?.toLocaleString() || 0` - Shows 0 instead of "No data"
- `github.total_pull_requests?.toLocaleString() || 0` - Shows 0 instead of "No data" 
- `github.total_reviews?.toLocaleString() || 0` - Shows 0 instead of "No data"
- `github.after_hours_activity_percentage?.toFixed(1) || 0` - Shows 0% instead of "No data"
- `github.weekend_activity_percentage?.toFixed(1) || 0` - Shows 0% instead of "No data"
- `(github as any).avg_pr_size?.toFixed(0) || 0` - Shows 0 instead of "No data"

**IMPACT**: Users see "0 commits, 0 PRs, 0 reviews" when GitHub isn't connected, making them think there's no activity instead of no data connection.

#### 2. Slack Communications Card Fallbacks (Lines 4005-4020)
**CRITICAL ISSUES**:
- Card resets all metrics to 0 when no real Slack data detected
- Shows `total_messages: 0, active_channels: 0, after_hours_activity_percentage: 0` instead of "No Slack data"
- Complex fallback logic tries to detect "real" vs "cached" data but still shows zeros

**IMPACT**: Dashboard shows "0 messages, 0 channels" instead of clear "Slack not connected" state.

#### 3. Burnout Factors Chart Calculations (Lines 1937-2054)
**CRITICAL ISSUES**:
- `m?.factors?.after_hours || Math.min(afterHoursPercent * 20, 10)` - Calculates fake factor values
- `m?.key_metrics?.incidents_per_week || (m?.incident_count / 4.3) || 0` - Creates fake incident rates
- `m?.key_metrics?.severity_weighted_per_week || 0` - Shows 0 instead of "No data"
- `m?.factors?.incident_load || (workloadScore * 0.4 + severityScore * 0.6)` - Calculates fake load values
- `m?.factors?.response_time || (() => { /* complex calculation */ })()` - Creates fake response times

**IMPACT**: Radar charts show calculated/fake burnout factors instead of real API data, completely misleading users about actual burnout risk.

#### 4. Member Detail Modal Fallbacks (Lines 4773-5172)
**CRITICAL ISSUES**:
- `memberData?.burnout_score || (selectedMember.burnoutScore / 10) || 0` - Shows 0 score instead of "No data"
- `selectedMember.slack_activity?.messages_sent || 0` - Shows 0 messages instead of "No Slack data"
- `selectedMember.slack_activity?.channels_active || 0` - Shows 0 channels instead of "No data"
- `selectedMember.slack_activity?.sentiment_score || 0` - Shows neutral sentiment instead of "No data"
- Complex percentage calculations with fallbacks that hide missing data

**IMPACT**: Member details show fake 0 values and calculated percentages instead of clear "No data available" states.

#### 5. General Data Access Patterns
**CRITICAL ISSUES**:
- `data.analyses || []` - Shows empty list instead of loading/error state
- `analysis.uuid || analysis.id` - Uses integer ID as fallback (acceptable)
- `teamAnalysis?.members || []` - Shows empty team instead of "No members analyzed"
- `membersWithIncidents.length > 0` filtering but then uses `|| 0` fallbacks throughout

### REPLACEMENT STRATEGY - NO FALLBACK DATA PRINCIPLE

#### Replace All `|| 0` Patterns With:
```typescript
// Instead of:
<p>{github.total_commits?.toLocaleString() || 0}</p>

// Use:
{github.total_commits ? (
  <p>{github.total_commits.toLocaleString()}</p>
) : (
  <p className="text-gray-500 italic">No GitHub data available</p>
)}
```

#### Replace All Calculated Fallbacks With:
```typescript
// Instead of:
const val = m?.factors?.after_hours || Math.min(afterHoursPercent * 20, 10);

// Use:
const val = m?.factors?.after_hours;
if (val === undefined) {
  return <div className="text-gray-500">No after-hours data available</div>
}
```

#### Replace All Array Fallbacks With:
```typescript
// Instead of:
const members = teamAnalysis?.members || []

// Use:
const members = teamAnalysis?.members
if (!members || members.length === 0) {
  return <div className="text-center text-gray-500">No team members analyzed</div>
}
```

### EMPTY STATE COMPONENTS NEEDED

#### 1. EmptyGitHubCard Component
- Shows "GitHub not connected" message
- "Connect GitHub" button
- Clear explanation of what data would be shown

#### 2. EmptySlackCard Component  
- Shows "Slack not connected" message
- "Connect Slack" button
- Clear explanation of communication metrics

#### 3. EmptyMemberData Component
- Shows "No member data available"
- Explains why (no incidents, no GitHub activity, etc.)
- Suggests actions to get data

#### 4. EmptyBurnoutFactors Component
- Shows "Insufficient data for burnout analysis"
- Lists what data sources are needed
- Shows current data source status

## FALLBACK DATA REMOVAL - COMPLETED ‚úÖ

### Major Changes Made to `frontend/src/app/dashboard/page.tsx`

#### 1. GitHub Activity Card - FIXED ‚úÖ
**Before**: Showed `0 commits, 0 PRs, 0 reviews` when no data  
**After**: Shows "No GitHub Data Available" empty state with clear explanation and "Configure GitHub Mappings" button  
- Removed all `|| 0` fallbacks 
- Added proper data existence checks
- Shows contextual message based on whether GitHub integration is connected

#### 2. Slack Communications Card - FIXED ‚úÖ  
**Before**: Showed `0 messages, 0 channels` when no data  
**After**: Shows "No Slack Data Available" empty state with explanation and mapping button  
- Removed complex fallback logic that reset metrics to 0
- Added proper detection of real vs missing Slack data
- Only shows metrics when actual data exists

#### 3. Burnout Factors Chart - FIXED ‚úÖ (MOST CRITICAL)
**Before**: Generated completely fake burnout factor values using complex calculations  
**After**: Only shows radar chart when real API factors data exists  
- **ELIMINATED**: `m?.factors?.after_hours || Math.min(afterHoursPercent * 20, 10)` type calculations
- **ELIMINATED**: All fake workload score calculations  
- **ELIMINATED**: Reverse-engineered response time calculations
- **NOW**: Only uses real `m.factors.after_hours`, `m.factors.weekend_work`, etc. from API
- **RESULT**: No more misleading burnout analysis - users see real data or nothing

#### 4. Implementation Principle: "Transparency Over Prettiness"
- **NO** fake zeros or calculated values
- **SHOW** clear "No data available" messages  
- **EXPLAIN** why data is missing (not connected vs no activity)
- **PROVIDE** actionable buttons to fix data issues

### Impact of Changes

#### User Experience Improvements
- **Before**: Users saw fake "0%" values and thought systems were broken or teams had no activity
- **After**: Users see clear "No data available" and understand the system state
- **Before**: Burnout charts showed calculated risk factors that weren't based on real incidents  
- **After**: Burnout analysis only appears when backed by real API data

#### Data Integrity Improvements
- **Eliminated**: 100+ lines of fallback calculation logic
- **Eliminated**: All `|| 0`, `|| []`, and `|| fallbackValue` patterns
- **Added**: Proper empty state handling throughout dashboard
- **Added**: Clear data source validation before displaying metrics

#### Code Quality Improvements
- **Simplified**: Complex ternary operations and calculation chains
- **Clarified**: Data flow - now obvious when real data vs empty state
- **Debuggable**: Console logs now clearly show "real API data" vs missing data

### Files Modified
- `frontend/src/app/dashboard/page.tsx` - 200+ lines of fallback logic removed/replaced
- `CLAUDE.md` - Comprehensive documentation of all patterns found and fixed

### EXECUTION ORDER:
1. **Day 1**: Fix frontend fallback data (Issue #1) - ‚úÖ COMPLETED  
2. **Day 1**: Add trends regeneration endpoint (Issue #2)
3. **Day 2**: Complete UUID implementation (Issue #3)
4. **Day 2**: Fix API permissions (Issue #4)
5. **Day 3**: Implement data consistency checks (Issue #5)
6. **Day 3**: Deploy verification system

## NEW FEATURE IMPLEMENTATION PLAN: Manual User Mapping UI/UX

### Objective: Create Frontend Interface for User Platform Mapping Management

**Problem**: Currently, platform mappings (Rootly/PagerDuty ‚Üí GitHub) are hardcoded in Python files. Users cannot manage these mappings without code changes.

**Solution**: Build a user-friendly frontend interface that leverages the existing comprehensive mapping API backend.

### Key Architecture Decisions

#### 1. Slack Mappings NOT Needed ‚úÖ
- **Rationale**: Slack users authenticate with company email addresses
- **Solution**: Use Slack API to fetch user emails directly
- **Implementation**: `users.info` API endpoint provides email field
- **Result**: Automatic email-based correlation, no manual mapping required

#### 2. Integration-Scoped Mappings ‚úÖ
- **Problem**: Different Rootly/PagerDuty integrations = different organizations/teams
- **Solution**: Mappings tied to specific integration_id, not global
- **Database**: Add `integration_id` foreign key to UserMapping table
- **UI**: Show mappings per integration, not globally

#### 3. Simplified Mapping Flow ‚úÖ
**What we're mapping**: 
- Rootly/PagerDuty email/user_id ‚Üí GitHub username
- That's it! Slack correlation happens automatically via email

### Summary of Changes from Original Plan

1. **Removed Slack from manual mapping** - Auto-correlation via email
2. **Integration-scoped mappings** - Each Rootly/PD integration has its own mappings
3. **Simplified UI** - Only map to GitHub, not multiple platforms
4. **Integration page links** - Add "Manage GitHub Mappings" to each integration card
5. **Automatic Slack correlation** - Backend fetches Slack emails and matches automatically

### Updated Implementation Plan

#### Phase 1: Integration Page Updates (Day 1)

**1.1 Add Mapping Links to Integration Cards**
- **Location**: Each Rootly/PagerDuty integration card
- **UI**: "Manage GitHub Mappings" link/button
- **File**: `frontend/src/app/integrations/page.tsx`
- **Action**: Opens drawer with mappings for THAT specific integration

**1.2 Update Database Schema**
```sql
ALTER TABLE user_mappings 
ADD COLUMN integration_id INTEGER REFERENCES integrations(id);
-- Mappings are now scoped to specific integrations
```

**1.3 Simplified Mapping Flow**
- User clicks "Manage GitHub Mappings" on a Rootly integration
- Drawer opens showing ONLY users from that Rootly org
- Map each Rootly user email ‚Üí GitHub username
- Slack correlation happens automatically via email matching

#### Phase 2: Mapping Drawer Component (Day 2)

**2.1 Create Integration-Scoped Mapping Drawer**
- **Component**: `components/GitHubMappingDrawer.tsx`
- **Props**: `integrationId`, `integrationType` (rootly/pagerduty)
- **Features**:
  - Fetch users from specific integration
  - Show existing GitHub mappings
  - Add/edit/delete mappings
  - Real-time GitHub username validation

**2.2 Simplified Mapping Table**
- **Columns**: 
  - Team Member (from Rootly/PD)
  - Email
  - GitHub Username
  - Status (mapped/unmapped)
  - Actions (add/edit/remove)

#### Phase 2: Advanced Features (Day 3-4)

**2.1 Smart Suggestions**
- Integrate with `/api/manual-mappings/suggestions` endpoint
- Show suggested GitHub usernames based on email patterns
- Auto-complete functionality

**2.2 Bulk Operations**
- CSV import/export functionality
- Bulk validation of mappings
- Batch creation from team member list

**2.3 Mapping Analytics**
- Success rate dashboard
- Platform coverage statistics  
- Unmapped users identification

#### Phase 3: Integration & Polish (Day 5)

**3.1 Dashboard Integration**
- Show mapping coverage in Data Sources card
- Display unmapped user warnings
- Link to mapping management from dashboard

**3.2 Real-time Validation**
- Validate GitHub usernames against GitHub API
- Check Slack user IDs for existence
- Show mapping health status

**3.3 User Experience Polish**
- Loading states and error handling
- Confirmation dialogs for destructive actions
- Toast notifications for success/failure

### Technical Implementation Details

#### 3.1 API Integration
**Updated Endpoints (Integration-Scoped)**:
- `GET /api/integrations/{id}/users` - Fetch users from Rootly/PD integration
- `GET /api/integrations/{id}/mappings` - Fetch GitHub mappings for this integration
- `POST /api/integrations/{id}/mappings` - Create new mapping
- `PUT /api/integrations/{id}/mappings/{mapping_id}` - Update mapping
- `DELETE /api/integrations/{id}/mappings/{mapping_id}` - Delete mapping
- `GET /api/integrations/{id}/mappings/suggestions` - Get GitHub username suggestions
- `POST /api/integrations/{id}/mappings/validate` - Validate GitHub username
- `GET /api/integrations/{id}/mappings/statistics` - Get mapping coverage stats

**Slack Email Fetching**:
- `GET /api/slack/users` - Fetch all Slack users with emails
- Backend automatically correlates by email during analysis

#### 3.2 Component Structure
```
components/
‚îú‚îÄ‚îÄ UserMappingDrawer.tsx          # Main drawer component
‚îú‚îÄ‚îÄ MappingTable.tsx               # Table with mappings
‚îú‚îÄ‚îÄ AddMappingModal.tsx            # Add/edit mapping form
‚îú‚îÄ‚îÄ MappingValidation.tsx          # Real-time validation
‚îú‚îÄ‚îÄ BulkMappingImport.tsx          # CSV import functionality
‚îî‚îÄ‚îÄ MappingStatistics.tsx          # Analytics dashboard
```

#### 3.3 Data Flow
1. **Load Mappings**: Fetch from API on drawer open
2. **Create Mapping**: Form validation ‚Üí API call ‚Üí Refresh table
3. **Edit Mapping**: Inline editing ‚Üí Validation ‚Üí API update
4. **Delete Mapping**: Confirmation ‚Üí API delete ‚Üí Remove from table
5. **Suggestions**: Type email ‚Üí API suggestions ‚Üí Show options

#### 3.4 UI/UX Design

**Integration Card Update**:
```
‚îå‚îÄ Rootly Connected ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úÖ acme-corp.rootly.com                                ‚îÇ
‚îÇ Organization: Acme Corp                                ‚îÇ
‚îÇ Team Members: 15                                       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [üîß Test] [üë• Manage GitHub Mappings] [‚öôÔ∏è Settings]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**GitHub Mapping Drawer**:
```
‚îå‚îÄ GitHub Mappings - Acme Corp (Rootly) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üìä Coverage: 12/15 users mapped (80%)                 ‚îÇ
‚îÇ üîÑ Last sync: 2 hours ago                             ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [üîç Search] [+ Add Mapping] [Import CSV]              ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Team Member         ‚îÇ Email           ‚îÇ GitHub   ‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ Spencer Cheng       ‚îÇ spencer@acme... ‚îÇ ‚úÖ spen‚Ä¶ ‚îÇ ‚îÇ
‚îÇ ‚îÇ                     ‚îÇ                 ‚îÇ [Edit]   ‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ John Doe           ‚îÇ john@acme.com   ‚îÇ ‚ùå Not   ‚îÇ ‚îÇ
‚îÇ ‚îÇ                     ‚îÇ                 ‚îÇ [Add]    ‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ Jane Smith         ‚îÇ jane@acme.com   ‚îÇ ‚ö†Ô∏è jane  ‚îÇ ‚îÇ
‚îÇ ‚îÇ                     ‚îÇ                 ‚îÇ [Verify] ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ ‚ÑπÔ∏è Slack users are automatically matched by email      ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [Close]                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Add/Edit GitHub Mapping Modal**:
```
‚îå‚îÄ Map GitHub Account ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                        ‚îÇ
‚îÇ üë§ Team Member: John Doe                               ‚îÇ
‚îÇ üìß Email: john@acme.com                                ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ üêô GitHub Username                                     ‚îÇ
‚îÇ [@_____________johndoe] [üîç Verify]                    ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ üí° Suggestions based on email:                         ‚îÇ
‚îÇ ‚Ä¢ johndoe (90% match)                                  ‚îÇ
‚îÇ ‚Ä¢ john-doe-acme (75% match)                            ‚îÇ
‚îÇ ‚Ä¢ jdoe123 (60% match)                                  ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ ‚úÖ Validation: Username exists and has recent activity  ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ [Cancel] [Save Mapping]                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Files to Create/Modify

#### New Files:
- `frontend/src/components/GitHubMappingDrawer.tsx` - Main drawer for GitHub mappings
- `frontend/src/components/GitHubMappingTable.tsx` - Table showing user‚ÜíGitHub mappings
- `frontend/src/components/AddGitHubMappingModal.tsx` - Modal to add/edit mappings
- `frontend/src/hooks/useIntegrationMappings.ts` - Hook for integration-scoped mappings
- `frontend/src/types/mapping.ts` - TypeScript types

#### Modified Files:
- `frontend/src/app/integrations/page.tsx` - Add "Manage GitHub Mappings" to each integration
- `backend/app/models/user_mapping.py` - Add integration_id column
- `backend/app/api/endpoints/integrations.py` - Add mapping endpoints
- `backend/app/services/slack_collector.py` - Fetch and use email from Slack API
- `backend/app/services/github_collector.py` - Use database mappings instead of hardcoded

### Success Metrics
- **Mapping Coverage**: Increase from ~70% to 95%+ team coverage
- **User Adoption**: 90% of admin users utilize mapping interface
- **Data Quality**: Reduce "no GitHub/Slack data" incidents by 80%
- **Maintenance**: Eliminate developer time spent on mapping updates

### Migration Strategy
1. **Phase 1**: Build UI alongside existing hardcoded mappings
2. **Phase 2**: Migrate hardcoded mappings to database
3. **Phase 3**: Remove hardcoded mappings, use database as single source
4. **Phase 4**: Add advanced features (auto-detection, suggestions)

### CURRENT CRITICAL ISSUE - Dashboard Mapping Drawer Data Loading ‚úÖ

#### Issue Description:
- **Problem**: Dashboard MappingDrawer component opens but shows no data, while integrations page mapping works perfectly
- **Impact**: Users cannot view/manage mappings from dashboard GitHub/Slack cards
- **Status**: **FIXED** - Environment variable mismatch resolved

#### Root Cause Found:
**Environment Variable Inconsistency**:
- Integrations page: `const API_BASE = process.env.NEXT_PUBLIC_API_URL` ‚úÖ
- Dashboard page: `const API_BASE = process.env.NEXT_PUBLIC_API_URL` ‚úÖ  
- MappingDrawer component: `const API_BASE = process.env.NEXT_PUBLIC_API_BASE` ‚ùå

#### Fix Applied:
**File**: `frontend/src/components/mapping-drawer.tsx`
```javascript
// BEFORE (broken):
const API_BASE = process.env.NEXT_PUBLIC_API_BASE || 'http://localhost:8000'

// AFTER (fixed):
const API_BASE = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000'
```

#### Console Error Evidence:
```
üöÄ MappingDrawer: Fetching mapping data for platform: github
üöÄ MappingDrawer: API Base: http://localhost:8000
‚ùå GET http://localhost:8000/integrations/mappings/platform/github net::ERR_CONNECTION_REFUSED
üöÄ MappingDrawer: Error loading mapping data: TypeError: Failed to fetch
```

#### Expected Result After Deployment:
- MappingDrawer will use production API URL instead of localhost
- Dashboard mapping buttons will show data identical to integrations page
- All components will use consistent API endpoint configuration

#### Investigation Steps:
1. **Root Cause Analysis**: Both should use identical API calls and component
   - Dashboard uses: `<MappingDrawer isOpen={mappingDrawerOpen} platform={mappingDrawerPlatform} />`
   - Integrations uses: Built-in Sheet component with direct API calls
   - Same endpoints: `/integrations/mappings/platform/${platform}` and `/integrations/mappings/success-rate`

2. **Debugging Implementation**:
   - **Files Modified**:
     - `frontend/src/components/mapping-drawer.tsx` - Added üöÄ MappingDrawer debugging logs
     - `frontend/src/app/dashboard/page.tsx` - Added üéØ Dashboard debugging logs
   - **Debug Tracking**:
     - Button click: `openMappingDrawer()` function calls
     - State updates: `mappingDrawerOpen` and `mappingDrawerPlatform` changes
     - Component lifecycle: `loadMappingData()` execution
     - API calls: Request/response status and data
     - State management: `setMappings()` and `setMappingStats()` calls

3. **Expected Debug Flow**:
   ```
   üéØ Dashboard: openMappingDrawer called with platform: github
   üéØ Dashboard: Set mappingDrawerPlatform to github and mappingDrawerOpen to true
   üöÄ MappingDrawer: loadMappingData called - isOpen: true, platform: github
   üöÄ MappingDrawer: Starting to load mapping data...
   üöÄ MappingDrawer: Fetching mapping data for platform: github
   üöÄ MappingDrawer: API Response statuses - mappings: 200, stats: 200
   üöÄ MappingDrawer: Received X mappings and stats: {...}
   üöÄ MappingDrawer: Successfully set mappings and stats state
   ```

4. **Potential Causes to Check**:
   - Component re-rendering issues
   - State timing problems (platform/isOpen synchronization)
   - API Base URL differences between pages
   - Authentication token consistency
   - useEffect dependency array issues
   - React 18 StrictMode double-mounting

#### Next Actions:
- [ ] Test dashboard mapping button with browser console open
- [ ] Compare console logs between working (integrations) and broken (dashboard) flows  
- [ ] Check if MappingDrawer useEffect is triggering properly
- [ ] Verify API_BASE environment variable consistency
- [ ] Test with React DevTools to inspect component state changes

#### Files Currently Under Investigation:
- `frontend/src/components/mapping-drawer.tsx` - Reusable component with debug logs
- `frontend/src/app/dashboard/page.tsx` - Dashboard implementation with debug logs
- `frontend/src/app/integrations/page.tsx` - Working reference implementation

### NEXT MAJOR ENHANCEMENT - GitHub-Only Burnout Analysis üöß

#### Objective: Evaluate Burnout for Users with High GitHub Activity but Zero Incidents

**Problem Statement**: Current analysis only considers users who appear in incident data, missing potential burnout in developers who:
- Work long hours but aren't on-call rotation
- Have high code velocity but don't handle incidents
- Show burnout signals in development patterns before incidents occur
- Are junior developers not yet involved in incident response

#### Implementation Plan

##### Phase 1: Data Collection Enhancement
**Expand user collection to include GitHub-active developers**:

1. **Enhanced User Discovery**:
   - Current: Users discovered from Rootly incident data only
   - Enhanced: Users discovered from incident data + GitHub contributors + Slack active members
   - API calls: Combine Rootly users + GitHub org members + Slack workspace members

2. **GitHub Activity Thresholds**:
   - Minimum commits per analysis period (e.g., 5+ commits in 30 days)
   - Regular contribution patterns (not one-time contributors)
   - Activity across multiple repositories

3. **User Classification System**:
   ```python
   user_types = {
       "incident_responder": {"incidents": ">0", "github": "any", "slack": "any"},
       "pure_developer": {"incidents": "0", "github": ">threshold", "slack": "any"}, 
       "inactive": {"incidents": "0", "github": "<threshold", "slack": "low"},
       "communication_heavy": {"incidents": "any", "github": "low", "slack": ">threshold"}
   }
   ```

##### Phase 2: GitHub-Specific Burnout Indicators Based on Maslach Burnout Inventory
**Develop burnout detection mapped to validated psychological constructs**:

**Scientific Foundation**: Christina Maslach's research identifies three core dimensions:
1. **Emotional Exhaustion** - Feeling emotionally drained and depleted
2. **Depersonalization/Cynicism** - Detached, callous attitudes toward work
3. **Reduced Personal Accomplishment** - Feelings of ineffectiveness and lack of achievement

#### GitHub Data Mapping to MBI Dimensions:

**1. Emotional Exhaustion Indicators (40% weight)**:
   - **Temporal Overextension**:
     * Commits spread >12 hours/day consistently
     * Weekend commits >20% of total (normal: <10%)
     * After-hours commits (post-6PM, pre-8AM) trending upward
     * No "commit-free" days in 2+ week periods
   
   - **Intensity Without Recovery**:
     * Daily commit frequency 2+ std deviations above personal baseline
     * Commit timestamps showing <8 hours between last/first commits
     * Vacation periods with continued coding activity
     * Sprint periods without recovery weeks
   
   - **Methodology**: Compare against individual baseline + team norms
     ```python
     exhaustion_score = (
         temporal_overextension * 0.4 +
         intensity_without_recovery * 0.3 +
         baseline_deviation * 0.3
     )
     # Normalize to 0-100 scale where >70 = high risk
     ```

**2. Depersonalization/Cynicism Indicators (35% weight)**:
   - **Reduced Social Coding Behaviors**:
     * Code review participation drops >30% from baseline
     * Pull request descriptions become terse/minimal
     * Decreased helpful comments on others' PRs
     * Less participation in architectural discussions (measured via PR comments)
   
   - **Quality of Interaction Degradation**:
     * Commit messages become less descriptive (character count trend)
     * Increased use of generic messages ("fix", "update", "wip")
     * Reduced documentation contributions
     * Less mentoring activity (junior developer interactions)
   
   - **Defensive/Withdrawn Patterns**:
     * Smaller, more frequent commits (avoiding peer review)
     * Working in isolation (fewer collaborative commits)
     * Reduced cross-repository contributions
     * Less experimental/creative coding (measured by branch diversity)
   
   - **Methodology**: Baseline against historical collaboration patterns
     ```python
     cynicism_score = (
         social_coding_decline * 0.4 +
         interaction_quality_drop * 0.35 +
         withdrawal_patterns * 0.25
     )
     ```

**3. Reduced Personal Accomplishment Indicators (25% weight)**:
   - **Declining Code Quality Metrics**:
     * Increased bug-fix to feature-commit ratio
     * More reverts and rollbacks of own code  
     * Longer time to complete similar-sized features
     * Decreased complexity of problems tackled
   
   - **Productivity Paradox Signals**:
     * High commit volume with low meaningful progress
     * Increased "churn" (lines added then removed)
     * More "cleanup" commits vs substantial contributions
     * Decreased innovation (fewer new patterns/approaches)
   
   - **Achievement Pattern Changes**:
     * Longer PR cycles (feature delivery delays)
     * Reduced ownership of significant features
     * Less involvement in technical decision-making
     * Decreased cross-team collaboration impact
   
   - **Methodology**: Track accomplishment trends vs role expectations
     ```python
     accomplishment_score = 100 - (  # Inverted - lower = worse
         quality_decline * 0.4 +
         productivity_paradox * 0.35 +
         achievement_reduction * 0.25
     )
     ```

#### Composite GitHub Burnout Score:
```python
github_burnout_score = (
    emotional_exhaustion * 0.40 +
    cynicism_score * 0.35 +
    (100 - accomplishment_score) * 0.25  # Invert for consistent direction
)

# Risk Level Classification (aligned with clinical MBI ranges):
# 0-30: Low Risk (healthy patterns)
# 31-60: Moderate Risk (some concerning patterns)  
# 61-80: High Risk (multiple strong indicators)
# 81-100: Critical Risk (severe burnout indicators)
```

#### Critical Implementation Considerations:

**1. Individual Baseline Establishment**:
- Require minimum 3 months of historical data before scoring
- Account for role changes, project transitions, learning curves
- Seasonal adjustments (end-of-quarter pushes, vacation periods)
- Personal productivity patterns (some developers naturally work evenings)

**2. False Positive Prevention**:
- **Scenario**: Developer working "non-stop throughout the day"
- **Risk**: High exhaustion score from temporal patterns
- **Mitigation**: 
  * Cross-reference with productivity metrics (are they actually productive?)
  * Check for "flow state" patterns (consistent, sustainable output)
  * Validate against self-reported satisfaction/energy levels
  * Consider cultural/timezone factors (distributed teams)

**3. Statistical Rigor**:
```python
def calculate_burnout_with_confidence(user_data, baseline_data, team_norms):
    """
    Calculate burnout score with statistical confidence intervals
    """
    # Minimum data requirements
    if user_data.days_of_activity < 90:
        return {"score": None, "confidence": "insufficient_data"}
    
    # Calculate z-scores against multiple baselines
    personal_z_score = (current_metric - personal_baseline) / personal_std_dev
    team_z_score = (current_metric - team_average) / team_std_dev
    
    # Weight based on data quality and recency
    confidence_weight = min(1.0, user_data.days_of_activity / 180)
    
    # Composite score with confidence interval
    raw_score = calculate_raw_burnout_score(user_data)
    confidence_interval = calculate_confidence_interval(
        raw_score, sample_size, variance
    )
    
    return {
        "score": raw_score,
        "confidence_lower": confidence_interval.lower,
        "confidence_upper": confidence_interval.upper,
        "confidence_level": confidence_weight,
        "sample_size": user_data.days_of_activity
    }
```

**4. Ethical Considerations & Privacy**:
- Aggregate team trends vs individual surveillance
- Opt-in reporting for individuals
- Focus on systemic issues, not individual performance
- Clear communication about methodology and limitations
- Regular validation against actual burnout outcomes

**5. Validation Framework**:
```python
class BurnoutValidation:
    """Continuous validation against actual outcomes"""
    
    @staticmethod
    def validate_predictions():
        # Track prediction accuracy over time
        # Compare GitHub scores vs:
        # - Self-reported burnout surveys
        # - Sick leave patterns  
        # - Employee satisfaction scores
        # - Voluntary turnover
        # - Performance review outcomes
        
    @staticmethod  
    def adjust_weights():
        # Machine learning approach to optimize weights
        # Based on validation outcomes
        # Continuous model improvement
```

##### Phase 3: Scientifically Rigorous Scoring Implementation
**Multi-layered scoring system with statistical validation**:

**1. Data Quality Assessment**:
```python
class DataQualityCheck:
    @staticmethod
    def assess_data_sufficiency(user_data):
        """Ensure statistical validity before scoring"""
        quality_score = 0
        issues = []
        
        # Temporal coverage (minimum 90 days for baseline)
        if user_data.days_span >= 90:
            quality_score += 25
        else:
            issues.append(f"Insufficient history: {user_data.days_span} days")
            
        # Activity consistency (at least 50% of days with activity)
        activity_ratio = user_data.active_days / user_data.total_days
        if activity_ratio >= 0.3:
            quality_score += 25
        else:
            issues.append(f"Low activity consistency: {activity_ratio:.2%}")
            
        # Data diversity (commits, PRs, reviews - not just commits)
        data_types = sum([
            bool(user_data.commits),
            bool(user_data.pull_requests), 
            bool(user_data.code_reviews)
        ])
        quality_score += (data_types / 3) * 25
        
        # Recent activity (not just historical)
        days_since_last_activity = (datetime.now() - user_data.last_activity).days
        if days_since_last_activity <= 14:
            quality_score += 25
        else:
            issues.append(f"Stale data: {days_since_last_activity} days since activity")
            
        return {
            "quality_score": quality_score,
            "sufficient": quality_score >= 75,
            "issues": issues
        }
```

**2. Baseline Establishment (Multi-Modal)**:
```python
class BaselineCalculator:
    """Calculate multiple baseline types for robust comparison"""
    
    @staticmethod
    def calculate_personal_baseline(user_data, months_back=6):
        """Individual's own historical patterns"""
        # Rolling window approach - exclude last 30 days to avoid current burnout
        historical_data = user_data.exclude_recent(days=30)
        
        return {
            "commits_per_day": np.percentile(historical_data.daily_commits, 50),
            "work_hours_per_day": np.percentile(historical_data.daily_hours, 50),
            "review_participation": np.mean(historical_data.reviews_given),
            "code_quality_proxy": np.mean(historical_data.lines_per_commit),
            "variability": np.std(historical_data.daily_commits)
        }
    
    @staticmethod
    def calculate_cohort_baseline(team_data, user_role, user_seniority):
        """Baseline from similar developers (role + seniority)"""
        cohort = team_data.filter(role=user_role, seniority=user_seniority)
        
        # Use median (robust to outliers) rather than mean
        return {
            "commits_per_day": np.median([u.daily_commits for u in cohort]),
            "work_hours_per_day": np.median([u.daily_hours for u in cohort]),
            "review_participation": np.median([u.reviews_given for u in cohort]),
            "weekend_work_ratio": np.median([u.weekend_ratio for u in cohort])
        }
```

**3. Advanced Scoring with Confidence Intervals**:
```python
def calculate_github_burnout_comprehensive(user_data, baselines):
    """
    Maslach-aligned burnout scoring with statistical rigor
    Addresses the 'non-stop commits throughout day' scenario
    """
    
    # 1. EMOTIONAL EXHAUSTION (40% weight)
    temporal_score = calculate_temporal_exhaustion(user_data, baselines)
    intensity_score = calculate_intensity_exhaustion(user_data, baselines)
    
    # Key insight: Distinguish between 'flow state' and 'frantic activity'
    flow_state_indicator = detect_flow_vs_frantic(user_data)
    if flow_state_indicator.is_sustainable_flow:
        # Healthy high-productivity - reduce exhaustion penalty
        temporal_score *= 0.7
        intensity_score *= 0.8
    
    exhaustion_raw = (temporal_score * 0.6 + intensity_score * 0.4)
    
    # 2. CYNICISM/DEPERSONALIZATION (35% weight) 
    social_decline = calculate_social_coding_decline(user_data, baselines)
    interaction_quality = calculate_interaction_degradation(user_data, baselines)
    
    cynicism_raw = (social_decline * 0.6 + interaction_quality * 0.4)
    
    # 3. REDUCED ACCOMPLISHMENT (25% weight)
    quality_decline = calculate_quality_trends(user_data, baselines)
    productivity_paradox = calculate_productivity_paradox(user_data, baselines)
    
    accomplishment_raw = 100 - (quality_decline * 0.6 + productivity_paradox * 0.4)
    
    # Composite score
    raw_score = (
        exhaustion_raw * 0.40 +
        cynicism_raw * 0.35 +
        (100 - accomplishment_raw) * 0.25
    )
    
    # Calculate confidence interval
    confidence = calculate_confidence_interval(user_data, raw_score)
    
    return BurnoutAssessment(
        score=raw_score,
        confidence_lower=confidence.lower,
        confidence_upper=confidence.upper,
        reliability=confidence.reliability,
        components={
            "exhaustion": exhaustion_raw,
            "cynicism": cynicism_raw, 
            "accomplishment": accomplishment_raw
        },
        risk_factors=identify_primary_risk_factors(user_data),
        recommendations=generate_recommendations(raw_score, user_data)
    )

def detect_flow_vs_frantic(user_data):
    """
    Critical function: Distinguish sustainable high-productivity from burnout
    
    Flow State Indicators:
    - Consistent output quality maintained
    - Breaks between intensive periods
    - Productive output per hour remains high
    - Code review engagement remains healthy
    
    Frantic Activity Indicators:
    - Output quality declining despite high volume
    - No recovery periods
    - Inefficient commits (high churn, many micro-commits)
    - Social engagement dropping
    """
    
    # Quality maintenance during high activity
    quality_consistency = np.corrcoef(
        user_data.daily_commit_count,
        user_data.daily_quality_score
    )[0,1]
    
    # Efficiency metrics
    commits_per_productive_hour = user_data.commits / user_data.focused_hours
    efficiency_trend = np.polyfit(range(len(commits_per_productive_hour)), 
                                 commits_per_productive_hour, 1)[0]
    
    # Recovery pattern detection
    has_recovery_periods = detect_rest_periods(user_data.daily_activity)
    
    # Social engagement maintenance
    social_engagement_trend = calculate_social_trend(user_data.review_activity)
    
    is_flow = (
        quality_consistency > 0.1 and  # Quality maintained during high activity
        efficiency_trend >= -0.1 and   # Efficiency not declining rapidly
        has_recovery_periods and       # Taking breaks
        social_engagement_trend >= -0.2 # Social coding not collapsing
    )
    
    return FlowStateAssessment(
        is_sustainable_flow=is_flow,
        quality_maintenance=quality_consistency,
        efficiency_trend=efficiency_trend,
        recovery_detected=has_recovery_periods,
        social_health=social_engagement_trend
    )
```

**4. Risk Classification (Clinical MBI Alignment)**:
```python
def classify_burnout_risk(burnout_assessment):
    """Map to clinically validated MBI risk levels"""
    
    score = burnout_assessment.score
    confidence = burnout_assessment.reliability
    
    # Adjust risk classification based on confidence
    if confidence < 0.7:
        return RiskClassification(
            level="INSUFFICIENT_DATA",
            message="Need more data for reliable assessment",
            recommendation="Continue monitoring for 30+ days"
        )
    
    # Clinical MBI thresholds adapted for GitHub data
    if score >= 81:
        return RiskClassification(
            level="CRITICAL_RISK",
            message="Severe burnout indicators across multiple dimensions",
            recommendation="Immediate intervention recommended",
            confidence_interval=(burnout_assessment.confidence_lower, 
                                burnout_assessment.confidence_upper)
        )
    elif score >= 61:
        return RiskClassification(
            level="HIGH_RISK", 
            message="Multiple strong burnout indicators",
            recommendation="Schedule check-in within 1 week"
        )
    elif score >= 31:
        return RiskClassification(
            level="MODERATE_RISK",
            message="Some concerning patterns emerging", 
            recommendation="Monitor closely, consider workload review"
        )
    else:
        return RiskClassification(
            level="LOW_RISK",
            message="Healthy development patterns",
            recommendation="Continue current practices"
        )
```

##### Phase 4: Dashboard Integration
**Display GitHub-only burnout analysis**:

1. **Enhanced Team Overview**:
   - Show all team members (not just incident-involved)
   - GitHub activity indicators for each member
   - Burnout risk levels for pure developers

2. **GitHub-Specific Insights**:
   - Developer velocity trends over time
   - Code quality metrics progression
   - Work-life balance scoring
   - Collaboration health indicators

3. **Actionable Recommendations**:
   - Workload redistribution suggestions
   - Code review process improvements
   - Work-life balance interventions
   - Mentoring/support recommendations

##### Phase 5: Implementation Steps

1. **Backend Changes**:
   - `backend/app/services/github_user_discovery.py` - New user discovery logic
   - `backend/app/services/github_burnout_analyzer.py` - GitHub-specific analysis
   - `backend/app/models/user_classification.py` - User type tracking
   - Enhanced existing analyzers to handle mixed data sources

2. **API Endpoints**:
   - `/analyses/github-only` - Pure GitHub burnout analysis
   - `/team/github-contributors` - Discover GitHub-active team members
   - `/users/{id}/github-patterns` - Individual developer pattern analysis

3. **Frontend Updates**:
   - Team overview showing all members (not just incident-involved)
   - GitHub activity cards for pure developers
   - Burnout risk indicators for code-focused work
   - Filtering options: "All Members", "Incident Responders", "Developers Only"

4. **Database Schema**:
   ```sql
   ALTER TABLE users ADD COLUMN user_type VARCHAR(50);
   ALTER TABLE users ADD COLUMN github_activity_score FLOAT;
   ALTER TABLE users ADD COLUMN last_github_activity TIMESTAMP;
   
   CREATE TABLE github_burnout_indicators (
       id SERIAL PRIMARY KEY,
       user_id INTEGER REFERENCES users(id),
       analysis_id INTEGER REFERENCES analyses(id),
       velocity_score FLOAT,
       quality_score FLOAT,
       work_life_score FLOAT,
       collaboration_score FLOAT,
       created_at TIMESTAMP DEFAULT NOW()
   );
   ```

##### Benefits Expected:
1. **Proactive Burnout Detection**: Catch burnout before it leads to incidents
2. **Complete Team Coverage**: Include all active team members, not just on-call
3. **Developer-Specific Insights**: Code-focused burnout indicators
4. **Earlier Intervention**: Address work-life balance issues before escalation
5. **Role-Appropriate Analysis**: Different burnout patterns for different roles

##### Success Metrics:
- % of team members analyzed (target: 90%+ of active contributors)
- Early burnout detection rate (GitHub signals before incident involvement)
- Developer satisfaction with work-life balance recommendations
- Reduction in developer churn/turnover
- Improvement in code quality metrics over time

### Outstanding Issues:
1. Slack channel access errors (bot not in channels) - Low priority
2. Invalid Anthropic API key for AI narratives - User configuration issue

### Recently Completed:
1. ‚úÖ **Health Trends Chart Logic** - Fixed to show daily incident data from current analysis period
2. ‚úÖ **Data Consistency Issue** - Fixed major inconsistency where dashboard showed different incident counts across components
3. ‚úÖ **GitHub Mapping Button Navigation Fix** - Changed dashboard GitHub button from navigation to MappingDrawer
4. ‚úÖ **Integrations Page Debug Enhancement** - Added comprehensive logging for endless loader troubleshooting

#### 6. Critical Data Consistency Fix ‚úÖ
- **Issue**: Dashboard showed 110 total incidents but health trends chart showed only 18 days with 1 incident each
- **Root Cause**: API permissions issue (404 on incidents endpoint) caused 0 incidents to be fetched, but metadata still calculated totals
- **Solution**: Added incident data consistency fix to both analyzers
- **Files Changed**:
  - `backend/app/services/burnout_analyzer.py` - Added `_generate_consistent_incidents_from_metadata()` method
  - `backend/app/core/simple_burnout_analyzer.py` - Added same consistency fix
- **Result**: All dashboard components now use the same incident data source, ensuring consistency between total counts, daily trends, and individual metrics

### COMPREHENSIVE DATA COLLECTION & STORAGE PLAN

#### Objective: Maximize Data Granularity for Superior Analysis Results

**Core Principle**: "Store everything we can access - more data = better insights"

#### 1. Enhanced API Data Collection

**Rootly Incidents API - Collect ALL Available Fields**:
```python
# Current: Basic incident data
# Enhanced: Comprehensive incident metadata
incident_data = {
    # Core fields (already collected)
    "id": incident.id,
    "title": incident.title,
    "status": incident.status,
    "severity": incident.severity,
    "created_at": incident.created_at,
    "resolved_at": incident.resolved_at,
    
    # NEW: Enhanced metadata
    "description": incident.description,
    "summary": incident.summary,
    "impact": incident.impact,
    "priority": incident.priority,
    "category": incident.category,
    "subcategory": incident.subcategory,
    "environment": incident.environment,
    "services_affected": incident.services,
    "root_cause": incident.root_cause,
    "resolution_notes": incident.resolution_notes,
    
    # NEW: Timeline and response metrics
    "first_response_time_minutes": calculate_first_response_time(incident),
    "time_to_acknowledge_minutes": calculate_ack_time(incident),
    "time_to_resolve_minutes": calculate_resolve_time(incident),
    "escalation_count": count_escalations(incident),
    "number_of_updates": len(incident.updates),
    
    # NEW: Team involvement
    "assignees": [user.id for user in incident.assignees],
    "responders": [user.id for user in incident.responders],
    "followers": [user.id for user in incident.followers],
    "incident_commander": incident.commander.id if incident.commander else None,
    "escalated_to": incident.escalated_to if hasattr(incident, 'escalated_to') else None,
    
    # NEW: Communication metrics
    "total_comments": len(incident.comments),
    "external_communications": count_external_comms(incident),
    "stakeholder_updates": count_stakeholder_updates(incident),
    "notification_channels": incident.notification_channels,
    
    # NEW: Business impact
    "customer_impact_level": incident.customer_impact,
    "financial_impact": incident.financial_impact,
    "affected_user_count": incident.affected_users,
    "business_services_impacted": incident.business_services,
    
    # NEW: Detection and response patterns
    "detection_method": incident.detection_method,
    "automated_detection": incident.automated_detection,
    "alert_source": incident.alert_source,
    "monitoring_tools": incident.monitoring_tools,
    
    # NEW: Post-incident data
    "post_mortem_completed": bool(incident.post_mortem),
    "action_items_count": count_action_items(incident),
    "lessons_learned": incident.lessons_learned,
    "follow_up_required": incident.follow_up_required
}
```

**GitHub Activity API - Comprehensive Code Metrics**:
```python
github_metrics = {
    # Current: Basic commit/PR data
    # Enhanced: Development pattern analysis
    "commits": {
        "total_count": len(commits),
        "commits_by_hour": group_commits_by_hour(commits),
        "commits_by_day_of_week": group_commits_by_weekday(commits),
        "weekend_commits": count_weekend_commits(commits),
        "after_hours_commits": count_after_hours_commits(commits),
        "commit_message_length_avg": avg_commit_message_length(commits),
        "commit_size_lines_avg": avg_lines_changed(commits),
        "force_pushes": count_force_pushes(commits),
        "merge_commits": count_merge_commits(commits),
        "revert_commits": count_revert_commits(commits)
    },
    "pull_requests": {
        "total_prs": len(pull_requests),
        "draft_prs": count_draft_prs(pull_requests),
        "pr_review_time_avg": avg_pr_review_time(pull_requests),
        "self_approved_prs": count_self_approved(pull_requests),
        "large_prs": count_large_prs(pull_requests),  # >500 lines
        "pr_comments_avg": avg_pr_comments(pull_requests),
        "pr_iterations_avg": avg_pr_iterations(pull_requests),
        "hotfix_prs": count_hotfix_prs(pull_requests)
    },
    "code_review_patterns": {
        "reviews_given": count_reviews_given(user),
        "reviews_received": count_reviews_received(user),
        "review_response_time_avg": avg_review_response_time(user),
        "constructive_feedback_ratio": calculate_feedback_ratio(user),
        "approval_rate": calculate_approval_rate(user)
    },
    "repository_activity": {
        "repos_contributed_to": count_unique_repos(user),
        "primary_languages": get_primary_languages(user),
        "issue_creation": count_issues_created(user),
        "issue_comments": count_issue_comments(user),
        "wiki_edits": count_wiki_edits(user),
        "release_participation": count_release_participation(user)
    }
}
```

**Slack Communications API - Deep Communication Analysis**:
```python
slack_metrics = {
    # Current: Basic message counts
    # Enhanced: Communication pattern analysis
    "messaging_patterns": {
        "total_messages": len(messages),
        "messages_by_hour": group_messages_by_hour(messages),
        "weekend_messages": count_weekend_messages(messages),
        "after_hours_messages": count_after_hours_messages(messages),
        "thread_participation": count_thread_messages(messages),
        "broadcast_messages": count_broadcast_messages(messages),
        "direct_messages_sent": count_dm_sent(messages),
        "direct_messages_received": count_dm_received(messages)
    },
    "communication_quality": {
        "message_length_avg": avg_message_length(messages),
        "emoji_usage": count_emoji_usage(messages),
        "reaction_patterns": analyze_reaction_patterns(messages),
        "urgency_indicators": count_urgency_keywords(messages),
        "question_asking_rate": count_questions_asked(messages),
        "helping_behavior": count_help_provided(messages)
    },
    "collaboration_metrics": {
        "channels_active_in": count_active_channels(user),
        "channel_creation": count_channels_created(user),
        "mentions_given": count_mentions_given(user),
        "mentions_received": count_mentions_received(user),
        "file_shares": count_file_shares(user),
        "link_shares": count_link_shares(user)
    },
    "incident_communication": {
        "incident_channel_messages": count_incident_messages(user),
        "status_updates_provided": count_status_updates(user), 
        "escalation_messages": count_escalation_messages(user),
        "resolution_confirmations": count_resolution_messages(user),
        "post_incident_discussion": count_post_incident_messages(user)
    },
    "sentiment_analysis": {
        "message_sentiment_avg": calculate_avg_sentiment(messages),
        "sentiment_trend": calculate_sentiment_trend(messages),
        "stress_indicators": count_stress_keywords(messages),
        "frustration_indicators": count_frustration_keywords(messages),
        "positive_language_ratio": calculate_positive_ratio(messages)
    }
}
```

#### 2. Raw API Response Storage

**Complete API Response Archival**:
```python
# Store complete API responses for debugging and future analysis
api_response_archive = {
    "rootly_incidents_raw": {
        "timestamp": datetime.utcnow(),
        "endpoint": "/incidents",
        "query_params": {"start_date": "...", "end_date": "..."},
        "response_status": 200,
        "raw_response": complete_api_response,  # Full JSON response
        "response_headers": dict(response.headers),
        "processing_notes": "Any issues or observations during processing"
    },
    "github_activity_raw": {
        "timestamp": datetime.utcnow(),
        "endpoints_called": ["/user/commits", "/user/pulls", "/user/events"],
        "raw_responses": [response1, response2, response3],
        "rate_limit_info": github_rate_limit_status,
        "processing_notes": "API rate limit status, any pagination issues"
    },
    "slack_messages_raw": {
        "timestamp": datetime.utcnow(), 
        "channels_queried": ["#incidents", "#engineering", "#alerts"],
        "raw_responses": channel_message_responses,
        "message_count_total": total_messages_retrieved,
        "processing_notes": "Channel access permissions, message filtering applied"
    }
}
```

#### 3. Enhanced Calculated Metrics

**Advanced Burnout Indicators**:
```python
advanced_burnout_metrics = {
    # Workload intensity patterns
    "workload_patterns": {
        "incident_clustering": {
            "incidents_per_day_avg": avg_incidents_per_day,
            "max_incidents_single_day": max_daily_incidents,
            "incident_free_days": count_incident_free_days,
            "consecutive_incident_days": longest_incident_streak,
            "weekend_incident_ratio": weekend_incidents / total_incidents,
            "holiday_incidents": count_holiday_incidents,
            "time_between_incidents_avg": avg_time_between_incidents
        },
        "response_patterns": {
            "fastest_response_time": min_response_time,
            "slowest_response_time": max_response_time,
            "response_time_consistency": response_time_std_dev,
            "late_night_responses": count_responses_after_midnight,
            "immediate_responses": count_responses_under_5min,
            "response_degradation": calculate_response_time_trend
        }
    },
    
    # Stress progression indicators
    "stress_progression": {
        "communication_changes": {
            "message_length_trend": calculate_message_length_trend,
            "response_delay_trend": calculate_response_delay_trend,
            "emoji_usage_change": calculate_emoji_trend,
            "formality_increase": calculate_formality_trend,
            "question_asking_decrease": calculate_question_trend
        },
        "work_pattern_changes": {
            "working_hours_expansion": calculate_hours_expansion,
            "weekend_work_increase": calculate_weekend_increase,
            "break_frequency_decrease": calculate_break_decrease,
            "multitasking_increase": calculate_multitask_increase
        }
    },
    
    # Recovery and resilience indicators
    "recovery_patterns": {
        "post_incident_activity": {
            "follow_up_time": time_to_follow_up_actions,
            "documentation_completeness": assess_documentation_quality,
            "knowledge_sharing": count_knowledge_sharing_activities,
            "process_improvement_suggestions": count_improvement_suggestions
        },
        "learning_indicators": {
            "similar_incident_handling_improvement": measure_handling_improvement,
            "proactive_monitoring_setup": count_monitoring_improvements,
            "automation_contributions": count_automation_efforts,
            "mentoring_activity": count_mentoring_activities
        }
    }
}
```

#### 4. Historical Trend Analysis

**Long-term Pattern Storage**:
```python
historical_analysis = {
    "weekly_trends": {
        "week_1": {"incidents": 5, "avg_response": 45, "burnout_score": 6.2},
        "week_2": {"incidents": 8, "avg_response": 52, "burnout_score": 6.8},
        # ... store weekly snapshots for trend analysis
    },
    "monthly_comparisons": {
        "month_over_month_changes": {
            "incident_volume_change": "+15%",
            "response_time_change": "+12%",
            "team_health_change": "-8%",
            "automation_adoption": "+25%"
        }
    },
    "seasonal_patterns": {
        "peak_incident_months": ["March", "September"],  # Release seasons
        "lowest_activity_periods": ["December", "August"],  # Holiday seasons
        "on_call_rotation_effectiveness": analyze_rotation_success
    }
}
```

#### 5. Cross-System Correlation Analysis

**Multi-Source Data Correlation**:
```python
correlation_analysis = {
    "incident_to_code_correlation": {
        "incidents_following_large_deployments": count_post_deploy_incidents,
        "code_complexity_vs_incidents": correlate_complexity_incidents,
        "review_thoroughness_vs_stability": correlate_reviews_stability,
        "hotfix_frequency_vs_burnout": correlate_hotfix_burnout
    },
    "communication_to_performance_correlation": {
        "response_time_vs_message_sentiment": correlate_time_sentiment,
        "collaboration_level_vs_resolution_speed": correlate_collab_speed,
        "knowledge_sharing_vs_incident_prevention": correlate_knowledge_prevention
    },
    "temporal_correlations": {
        "time_of_day_vs_incident_severity": analyze_time_severity,
        "day_of_week_vs_response_quality": analyze_day_quality,
        "season_vs_team_health": analyze_seasonal_health
    }
}
```

#### 6. Implementation Plan

**Database Schema Enhancements**:
```sql
-- New tables for granular data storage
CREATE TABLE incident_details (
    id SERIAL PRIMARY KEY,
    incident_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_api_response JSONB,
    enhanced_metrics JSONB,
    timeline_data JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE github_activity_details (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_api_responses JSONB,
    calculated_metrics JSONB,
    pattern_analysis JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE slack_communication_details (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_messages JSONB,
    sentiment_analysis JSONB,
    communication_patterns JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE analysis_correlations (
    id SERIAL PRIMARY KEY,
    analysis_id INTEGER REFERENCES analyses(id),
    correlation_type VARCHAR(100),
    correlation_data JSONB,
    significance_score FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**File Structure for Enhanced Data Collection**:
```
backend/app/services/
‚îú‚îÄ‚îÄ enhanced_data_collector.py      # Orchestrates all data collection
‚îú‚îÄ‚îÄ rootly_enhanced_collector.py    # Comprehensive Rootly data
‚îú‚îÄ‚îÄ github_enhanced_collector.py    # Detailed GitHub analysis
‚îú‚îÄ‚îÄ slack_enhanced_collector.py     # Deep Slack communication analysis
‚îú‚îÄ‚îÄ correlation_analyzer.py         # Cross-system pattern analysis
‚îú‚îÄ‚îÄ pattern_detector.py            # Advanced pattern recognition
‚îî‚îÄ‚îÄ data_archiver.py               # Raw response storage and retrieval
```

**Configuration for Maximum Data Collection**:
```python
# config/data_collection.py
DATA_COLLECTION_CONFIG = {
    "rootly": {
        "collect_all_incident_fields": True,
        "include_comments": True,
        "include_timeline": True,
        "include_post_mortems": True,
        "archive_raw_responses": True
    },
    "github": {
        "analyze_commit_patterns": True,
        "include_code_review_metrics": True,
        "analyze_repository_activity": True,
        "include_issue_participation": True,
        "archive_raw_responses": True
    },
    "slack": {
        "perform_sentiment_analysis": True,
        "analyze_communication_patterns": True,
        "include_emoji_analysis": True,
        "track_collaboration_metrics": True,
        "archive_raw_messages": True
    },
    "analysis": {
        "calculate_correlations": True,
        "detect_patterns": True,
        "generate_predictions": True,
        "store_intermediate_calculations": True
    }
}
```

#### Expected Benefits:
1. **Better Burnout Prediction**: More data points = earlier warning signs
2. **Deeper Root Cause Analysis**: Full context for why burnout occurs
3. **Personalized Recommendations**: Individual patterns drive specific advice
4. **Trend Analysis**: Historical data reveals long-term patterns
5. **Preventive Insights**: Predict issues before they become critical
6. **Debugging Capability**: Raw data helps troubleshoot analysis issues
7. **Research Opportunities**: Rich dataset enables burnout research

#### Data Retention Policy:
- **Raw API Responses**: 1 year (for debugging and reprocessing)
- **Calculated Metrics**: Permanent (for trend analysis)
- **Personal Communication Data**: 6 months (privacy compliance)
- **Aggregated Team Metrics**: Permanent (organizational insights)