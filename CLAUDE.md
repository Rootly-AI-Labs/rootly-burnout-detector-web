# Claude Development Session Notes

## Project: OnCall Burnout Detector - Railway Deployment & Analysis Fixes

### Recent Major Work Completed

#### 1. UUID Implementation for Unique Analysis URLs ✅
- **Issue**: Analyses used predictable integer IDs, not shareable
- **Solution**: Added UUID field to Analysis model with automatic generation
- **Files Changed**:
  - `backend/app/models/analysis.py` - Added UUID column
  - `backend/app/api/endpoints/analyses.py` - Added UUID endpoints & responses
  - `frontend/src/app/dashboard/page.tsx` - Updated to use UUIDs in URLs
- **Migration**: Created PostgreSQL migration scripts for Railway
- **Status**: Completed - analyses now have unique shareable URLs

#### 2. Railway PostgreSQL Compatibility Fixes ✅
- **Issue**: SQLAlchemy errors on Railway due to missing UUID column
- **Solution**: Graceful handling of missing columns during migration
- **Files Changed**:
  - `backend/migrate_uuid_step1.py` - Database migration script
  - `backend/app/api/endpoints/analyses.py` - Safe UUID access with getattr()
  - `frontend/src/app/dashboard/page.tsx` - Fallback to integer IDs
- **Status**: Completed - app works before and after migration

#### 3. Analysis Execution Error Fixes ✅
- **Issue**: Multiple "NoneType to numerator/denominator" errors
- **Solution**: Added None checks to all mathematical operations
- **Files Changed**:
  - `backend/app/services/burnout_analyzer.py` - Fixed division operations
  - `backend/app/core/burnout_analyzer.py` - Added None checks
  - `backend/app/services/unified_burnout_analyzer.py` - Protected calculations
  - `backend/app/agents/tools/*.py` - Added missing logger imports
- **Status**: Completed - analyses run without NoneType errors

#### 4. Burnout Factors Chart Data Fix ✅
- **Issue**: Chart showed all zeros despite hundreds of incidents
- **Solution**: Added missing 'factors' structure to SimpleBurnoutAnalyzer
- **Files Changed**:
  - `backend/app/core/simple_burnout_analyzer.py` - Added factors calculation
- **Status**: Completed - burnout factors now show actual values

#### 5. Historical Analyses Visibility Fix ✅
- **Issue**: Old analyses not showing due to UUID column errors
- **Solution**: Safe UUID field access with proper fallbacks
- **Files Changed**:
  - `backend/app/api/endpoints/analyses.py` - getattr() for safe access
- **Status**: Completed - all historical analyses visible

### CRITICAL ISSUE FIXED - Health Trends Chart Logic ✅

#### Issue Resolution:
The health trends chart was showing historical analysis results instead of daily incident data from the current analysis period.

#### Changes Made:
1. **Backend**: Modified `/analyses/trends/historical` endpoint in `backend/app/api/endpoints/analyses.py`
   - Now returns daily incident trends from the most recent analysis
   - Uses the `daily_trends` data already generated by the burnout analyzer
   - Each data point shows daily incident count, health score, and members at risk
   - Filters by `days_back` parameter for chart display range

2. **Data Structure**: The endpoint now returns:
   - Date for each day with incident data
   - Daily incident count and health scores
   - Members at risk based on incident patterns
   - Health status (critical, at_risk, moderate, healthy)
   - Timeline events for significant health changes

#### Technical Details:
- Uses most recent completed analysis instead of aggregating historical analyses
- Extracts `daily_trends` from analysis results (generated by `_generate_daily_trends`)
- Maintains same API response format for frontend compatibility
- Filters trends by date range specified in `days_back` parameter

### Testing Commands:
```bash
# Run backend
cd backend && python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Check database migrations needed on Railway
cd backend && python run_uuid_migration.py

# Debug trends data
cd backend && python debug_trends_data.py
```

### Known Working Features:
- ✅ Analysis creation and execution
- ✅ Burnout factors chart with real data
- ✅ Historical analyses list
- ✅ UUID-based shareable URLs
- ✅ Background analysis processing
- ✅ Error handling and recovery

### CRITICAL ISSUES TO FIX - EXECUTION PLAN

#### Issue #1: Frontend Shows Fallback/Demo Data for Non-Existent Analyses
**Problem**: URL shows `?analysis=64` but analysis doesn't exist, causing frontend to show hardcoded demo data (110 incidents, 18 days)
**Impact**: Users see inconsistent fake data that doesn't match any real analysis

**Fix Plan**:
1. **Frontend**: Add proper error handling when analysis not found
   - File: `frontend/src/app/dashboard/page.tsx`
   - Check if analysis exists before displaying data
   - Show clear error message: "Analysis not found" 
   - Redirect to most recent valid analysis or show empty state
   - Remove hardcoded fallback data (110 incidents, 18 days pattern)
   - **CRITICAL: NO FALLBACK DATA - Always show real state:**
     - If no data: "No incident data available"
     - If 0 incidents: "0 incidents analyzed" 
     - If API error: "Failed to fetch incidents: [error message]"
     - If daily trends empty: "No daily trend data generated"
     - NEVER show fake/demo data

2. **Backend**: Return proper 404 with redirect suggestion
   - File: `backend/app/api/endpoints/analyses.py`
   - When analysis not found, include most recent valid analysis ID in error response
   - Frontend can use this to auto-redirect
   - Include error details in response for frontend to display

#### Issue #2: Daily Trends Generation Completely Broken
**Problem**: ALL 78 completed analyses have 0 daily trends data, causing empty charts
**Impact**: Health trends chart shows no real data for any analysis

**Fix Plan**:
1. **Immediate Fix**: Add data regeneration endpoint
   - Create `/analyses/{id}/regenerate-trends` endpoint
   - Regenerate daily trends for existing analyses
   - Use the incident generation logic when API permissions blocked

2. **Root Cause Fix**: Ensure daily trends always generated
   - File: `backend/app/services/burnout_analyzer.py`
   - Add validation that daily_trends is never empty
   - If no incidents but time period exists, generate empty daily entries
   - Log warnings when daily trends generation fails

#### Issue #3: UUID Implementation Incomplete
**Problem**: UUID column commented out, using integer IDs, no shareable URLs
**Impact**: URLs not shareable, sequential IDs expose data

**Fix Plan**:
1. **Complete UUID Implementation**:
   - File: `backend/app/models/analysis.py`
   - Uncomment UUID column: `uuid = Column(String(36), unique=True, index=True, nullable=True, default=lambda: str(uuid.uuid4()))`
   - Commit and push the change

2. **Run Migration on Railway**:
   - The migration script already exists: `backend/migrate_uuid_step1.py`
   - Run via Railway CLI or web console
   - Verify UUID column added to all existing analyses

3. **Update Frontend to Use UUIDs**:
   - File: `frontend/src/app/dashboard/page.tsx`
   - Switch from `?analysis={id}` to `?analysis={uuid}`
   - Keep fallback to integer ID for backward compatibility

#### Issue #4: API Permissions Preventing Real Data
**Problem**: Rootly API returns 404 on incidents endpoint (missing incidents:read permission)
**Impact**: All analyses show 0 real incidents, only generated/fake data

**Fix Plan**:
1. **Update Rootly API Token**:
   - Get new token with 'incidents:read' permission from Rootly
   - Update in integration settings
   
2. **Add Permission Check on Integration Setup**:
   - File: `backend/app/api/endpoints/rootly.py`
   - Test permissions when saving integration
   - Show clear warning if incidents:read missing

#### Issue #5: Data Consistency Between Components
**Problem**: Different dashboard components show different data sources
**Impact**: Total incidents, charts, and metrics don't match

**Fix Plan**:
1. **Single Source of Truth**:
   - All components must read from analysis.results
   - Remove any hardcoded values
   - Add data validation to ensure consistency
   - **NO FALLBACK DATA PRINCIPLE**:
     - Every component shows EXACTLY what's in the database
     - If data is missing, show "Data not available" 
     - If calculation fails, show "Unable to calculate"
     - Log all issues for debugging

2. **Add Data Integrity Check**:
   - Create consistency validator
   - Run before displaying analysis
   - Log any discrepancies found
   - Display warnings to user when data inconsistencies detected

#### CORE PRINCIPLE: Transparency Over Prettiness
**Always show the real state of the system:**
- Empty data → Show empty state with explanation
- Failed API calls → Show error with details
- Missing permissions → Show "Insufficient permissions to access incidents"
- Broken calculations → Show "Calculation error" with details
- NEVER hide problems with fake data

### IMMEDIATE ACTIONS - Remove All Fallback Data

**Search and Remove These Patterns**:
1. **Frontend** (`frontend/src/app/dashboard/page.tsx`):
   - Any hardcoded "110 incidents"
   - Any "18 days" pattern
   - Default health scores when data missing
   - Mock timeline events
   - Placeholder member data
   
2. **Backend** (all analyzer files):
   - Remove any demo/mock data generation
   - Remove default values that hide real issues
   - Keep ONLY the incident generation for consistency (when metadata shows incidents but API fails)

**Replace With**:
```typescript
// Instead of: incidents = fallbackData || []
// Use: incidents = data?.incidents || []
// Display: {incidents.length === 0 ? "No incidents to display" : <IncidentChart />}
```

### DATA VERIFICATION & CROSS-CHECK PLAN

#### Automated Data Consistency Verification System

**1. Backend Validation Endpoint** (`/analyses/{id}/verify-consistency`):
```python
# Returns detailed consistency report
{
  "analysis_id": 140,
  "consistency_checks": {
    "incident_totals": {
      "metadata_total": 110,
      "team_analysis_sum": 108,
      "daily_trends_sum": 18,
      "match": false,
      "discrepancy": "Daily trends only sum to 18, not 110"
    },
    "member_counts": {
      "metadata_users": 38,
      "team_analysis_members": 38,
      "members_with_incidents": 5,
      "match": true
    },
    "date_ranges": {
      "metadata_days": 30,
      "daily_trends_days": 18,
      "expected_data_points": 30,
      "actual_data_points": 18,
      "match": false,
      "discrepancy": "Missing 12 days of daily trend data"
    },
    "severity_distribution": {
      "metadata_breakdown": {"SEV1": 3, "SEV2": 60, "SEV3": 16, "SEV4": 31},
      "calculated_total": 110,
      "incidents_with_severity": 110,
      "match": true
    }
  },
  "overall_consistency": false,
  "critical_issues": [
    "Daily trends incomplete: 18/30 days",
    "Daily incident sum (18) doesn't match total (110)"
  ]
}
```

**2. Frontend Consistency Dashboard** (Development Mode):
- Add debug panel showing all data sources
- Visual indicators when numbers don't match
- Color coding: ✅ Green (match), ❌ Red (mismatch), ⚠️ Yellow (close but off)

**3. Cross-Component Validation Rules**:

**Top Cards vs Source Data**:
```typescript
// Team Health Card
- Value shown: analysis.results.team_health.overall_score
- Validation: Must be between 0-100
- Cross-check: Average of member scores should be within 10% of overall

// At Risk Card  
- Value shown: analysis.results.team_health.members_at_risk
- Validation: Cannot exceed total members
- Cross-check: Count of members with risk_level !== 'low'

// Total Incidents Card
- Value shown: analysis.results.metadata.total_incidents
- Cross-checks:
  1. Sum of daily_trends[].incident_count
  2. Sum of team_analysis.members[].incident_count
  3. Sum of severity_breakdown values
  - ALL must match exactly
```

**Health Trends Chart vs Source Data**:
```typescript
// Each data point
- Date: daily_trends[i].date
- Score: daily_trends[i].overall_score
- Incidents: daily_trends[i].incident_count

// Validations:
- Number of points === metadata.days_analyzed
- Date range covers full analysis period
- Sum of incidents === metadata.total_incidents
- No duplicate dates
- Dates in chronological order
```

**4. Automated Test Suite**:
```python
# tests/test_data_consistency.py
def test_analysis_data_consistency():
    for analysis in get_all_completed_analyses():
        # Test 1: Incident totals match across all sources
        assert sum(day['incident_count'] for day in analysis.daily_trends) == analysis.metadata.total_incidents
        
        # Test 2: Daily trends cover full time period
        assert len(analysis.daily_trends) == analysis.metadata.days_analyzed
        
        # Test 3: Member incident counts sum to total
        member_incident_sum = sum(m['incident_count'] for m in analysis.team_analysis.members)
        assert member_incident_sum == analysis.metadata.total_incidents
        
        # Test 4: At-risk count matches actual risk levels
        at_risk_calculated = len([m for m in analysis.team_analysis.members if m['risk_level'] != 'low'])
        assert at_risk_calculated == analysis.team_health.members_at_risk
```

**5. Manual Verification Checklist** (For QA):
- [ ] Total Incidents card matches sum of severity breakdown
- [ ] Health trends chart shows correct number of days
- [ ] Each day's incidents sum to total when added
- [ ] Team health percentage matches calculation
- [ ] At-risk count matches highlighted members
- [ ] Timeline events correspond to daily trend data
- [ ] Burnout factors chart values sum correctly
- [ ] All dates match analysis time range

**6. Real-time Consistency Monitor**:
- Add console warnings when displaying inconsistent data
- Log discrepancies to monitoring system
- Email alert if consistency < 95% for new analyses

### EXECUTION ORDER:
1. **Day 1**: Fix frontend fallback data (Issue #1) - HIGHEST PRIORITY
2. **Day 1**: Add trends regeneration endpoint (Issue #2)
3. **Day 2**: Complete UUID implementation (Issue #3)
4. **Day 2**: Fix API permissions (Issue #4)
5. **Day 3**: Implement data consistency checks (Issue #5)
6. **Day 3**: Deploy verification system

## NEW FEATURE IMPLEMENTATION PLAN: Manual User Mapping UI/UX

### Objective: Create Frontend Interface for User Platform Mapping Management

**Problem**: Currently, platform mappings (Rootly/PagerDuty → GitHub) are hardcoded in Python files. Users cannot manage these mappings without code changes.

**Solution**: Build a user-friendly frontend interface that leverages the existing comprehensive mapping API backend.

### Key Architecture Decisions

#### 1. Slack Mappings NOT Needed ✅
- **Rationale**: Slack users authenticate with company email addresses
- **Solution**: Use Slack API to fetch user emails directly
- **Implementation**: `users.info` API endpoint provides email field
- **Result**: Automatic email-based correlation, no manual mapping required

#### 2. Integration-Scoped Mappings ✅
- **Problem**: Different Rootly/PagerDuty integrations = different organizations/teams
- **Solution**: Mappings tied to specific integration_id, not global
- **Database**: Add `integration_id` foreign key to UserMapping table
- **UI**: Show mappings per integration, not globally

#### 3. Simplified Mapping Flow ✅
**What we're mapping**: 
- Rootly/PagerDuty email/user_id → GitHub username
- That's it! Slack correlation happens automatically via email

### Summary of Changes from Original Plan

1. **Removed Slack from manual mapping** - Auto-correlation via email
2. **Integration-scoped mappings** - Each Rootly/PD integration has its own mappings
3. **Simplified UI** - Only map to GitHub, not multiple platforms
4. **Integration page links** - Add "Manage GitHub Mappings" to each integration card
5. **Automatic Slack correlation** - Backend fetches Slack emails and matches automatically

### Updated Implementation Plan

#### Phase 1: Integration Page Updates (Day 1)

**1.1 Add Mapping Links to Integration Cards**
- **Location**: Each Rootly/PagerDuty integration card
- **UI**: "Manage GitHub Mappings" link/button
- **File**: `frontend/src/app/integrations/page.tsx`
- **Action**: Opens drawer with mappings for THAT specific integration

**1.2 Update Database Schema**
```sql
ALTER TABLE user_mappings 
ADD COLUMN integration_id INTEGER REFERENCES integrations(id);
-- Mappings are now scoped to specific integrations
```

**1.3 Simplified Mapping Flow**
- User clicks "Manage GitHub Mappings" on a Rootly integration
- Drawer opens showing ONLY users from that Rootly org
- Map each Rootly user email → GitHub username
- Slack correlation happens automatically via email matching

#### Phase 2: Mapping Drawer Component (Day 2)

**2.1 Create Integration-Scoped Mapping Drawer**
- **Component**: `components/GitHubMappingDrawer.tsx`
- **Props**: `integrationId`, `integrationType` (rootly/pagerduty)
- **Features**:
  - Fetch users from specific integration
  - Show existing GitHub mappings
  - Add/edit/delete mappings
  - Real-time GitHub username validation

**2.2 Simplified Mapping Table**
- **Columns**: 
  - Team Member (from Rootly/PD)
  - Email
  - GitHub Username
  - Status (mapped/unmapped)
  - Actions (add/edit/remove)

#### Phase 2: Advanced Features (Day 3-4)

**2.1 Smart Suggestions**
- Integrate with `/api/manual-mappings/suggestions` endpoint
- Show suggested GitHub usernames based on email patterns
- Auto-complete functionality

**2.2 Bulk Operations**
- CSV import/export functionality
- Bulk validation of mappings
- Batch creation from team member list

**2.3 Mapping Analytics**
- Success rate dashboard
- Platform coverage statistics  
- Unmapped users identification

#### Phase 3: Integration & Polish (Day 5)

**3.1 Dashboard Integration**
- Show mapping coverage in Data Sources card
- Display unmapped user warnings
- Link to mapping management from dashboard

**3.2 Real-time Validation**
- Validate GitHub usernames against GitHub API
- Check Slack user IDs for existence
- Show mapping health status

**3.3 User Experience Polish**
- Loading states and error handling
- Confirmation dialogs for destructive actions
- Toast notifications for success/failure

### Technical Implementation Details

#### 3.1 API Integration
**Updated Endpoints (Integration-Scoped)**:
- `GET /api/integrations/{id}/users` - Fetch users from Rootly/PD integration
- `GET /api/integrations/{id}/mappings` - Fetch GitHub mappings for this integration
- `POST /api/integrations/{id}/mappings` - Create new mapping
- `PUT /api/integrations/{id}/mappings/{mapping_id}` - Update mapping
- `DELETE /api/integrations/{id}/mappings/{mapping_id}` - Delete mapping
- `GET /api/integrations/{id}/mappings/suggestions` - Get GitHub username suggestions
- `POST /api/integrations/{id}/mappings/validate` - Validate GitHub username
- `GET /api/integrations/{id}/mappings/statistics` - Get mapping coverage stats

**Slack Email Fetching**:
- `GET /api/slack/users` - Fetch all Slack users with emails
- Backend automatically correlates by email during analysis

#### 3.2 Component Structure
```
components/
├── UserMappingDrawer.tsx          # Main drawer component
├── MappingTable.tsx               # Table with mappings
├── AddMappingModal.tsx            # Add/edit mapping form
├── MappingValidation.tsx          # Real-time validation
├── BulkMappingImport.tsx          # CSV import functionality
└── MappingStatistics.tsx          # Analytics dashboard
```

#### 3.3 Data Flow
1. **Load Mappings**: Fetch from API on drawer open
2. **Create Mapping**: Form validation → API call → Refresh table
3. **Edit Mapping**: Inline editing → Validation → API update
4. **Delete Mapping**: Confirmation → API delete → Remove from table
5. **Suggestions**: Type email → API suggestions → Show options

#### 3.4 UI/UX Design

**Integration Card Update**:
```
┌─ Rootly Connected ─────────────────────────────────────┐
│ ✅ acme-corp.rootly.com                                │
│ Organization: Acme Corp                                │
│ Team Members: 15                                       │
│                                                        │
│ [🔧 Test] [👥 Manage GitHub Mappings] [⚙️ Settings]    │
└────────────────────────────────────────────────────────┘
```

**GitHub Mapping Drawer**:
```
┌─ GitHub Mappings - Acme Corp (Rootly) ─────────────────┐
│ 📊 Coverage: 12/15 users mapped (80%)                 │
│ 🔄 Last sync: 2 hours ago                             │
│                                                        │
│ [🔍 Search] [+ Add Mapping] [Import CSV]              │
│                                                        │
│ ┌────────────────────────────────────────────────────┐ │
│ │ Team Member         │ Email           │ GitHub   │ │
│ ├────────────────────────────────────────────────────┤ │
│ │ Spencer Cheng       │ spencer@acme... │ ✅ spen… │ │
│ │                     │                 │ [Edit]   │ │
│ ├────────────────────────────────────────────────────┤ │
│ │ John Doe           │ john@acme.com   │ ❌ Not   │ │
│ │                     │                 │ [Add]    │ │
│ ├────────────────────────────────────────────────────┤ │
│ │ Jane Smith         │ jane@acme.com   │ ⚠️ jane  │ │
│ │                     │                 │ [Verify] │ │
│ └────────────────────────────────────────────────────┘ │
│                                                        │
│ ℹ️ Slack users are automatically matched by email      │
│                                                        │
│ [Close]                                                │
└────────────────────────────────────────────────────────┘
```

**Add/Edit GitHub Mapping Modal**:
```
┌─ Map GitHub Account ───────────────────────────────────┐
│                                                        │
│ 👤 Team Member: John Doe                               │
│ 📧 Email: john@acme.com                                │
│                                                        │
│ 🐙 GitHub Username                                     │
│ [@_____________johndoe] [🔍 Verify]                    │
│                                                        │
│ 💡 Suggestions based on email:                         │
│ • johndoe (90% match)                                  │
│ • john-doe-acme (75% match)                            │
│ • jdoe123 (60% match)                                  │
│                                                        │
│ ✅ Validation: Username exists and has recent activity  │
│                                                        │
│ [Cancel] [Save Mapping]                                │
└────────────────────────────────────────────────────────┘
```

### Files to Create/Modify

#### New Files:
- `frontend/src/components/GitHubMappingDrawer.tsx` - Main drawer for GitHub mappings
- `frontend/src/components/GitHubMappingTable.tsx` - Table showing user→GitHub mappings
- `frontend/src/components/AddGitHubMappingModal.tsx` - Modal to add/edit mappings
- `frontend/src/hooks/useIntegrationMappings.ts` - Hook for integration-scoped mappings
- `frontend/src/types/mapping.ts` - TypeScript types

#### Modified Files:
- `frontend/src/app/integrations/page.tsx` - Add "Manage GitHub Mappings" to each integration
- `backend/app/models/user_mapping.py` - Add integration_id column
- `backend/app/api/endpoints/integrations.py` - Add mapping endpoints
- `backend/app/services/slack_collector.py` - Fetch and use email from Slack API
- `backend/app/services/github_collector.py` - Use database mappings instead of hardcoded

### Success Metrics
- **Mapping Coverage**: Increase from ~70% to 95%+ team coverage
- **User Adoption**: 90% of admin users utilize mapping interface
- **Data Quality**: Reduce "no GitHub/Slack data" incidents by 80%
- **Maintenance**: Eliminate developer time spent on mapping updates

### Migration Strategy
1. **Phase 1**: Build UI alongside existing hardcoded mappings
2. **Phase 2**: Migrate hardcoded mappings to database
3. **Phase 3**: Remove hardcoded mappings, use database as single source
4. **Phase 4**: Add advanced features (auto-detection, suggestions)

### CURRENT CRITICAL ISSUE - Dashboard Mapping Drawer Data Loading ✅

#### Issue Description:
- **Problem**: Dashboard MappingDrawer component opens but shows no data, while integrations page mapping works perfectly
- **Impact**: Users cannot view/manage mappings from dashboard GitHub/Slack cards
- **Status**: **FIXED** - Environment variable mismatch resolved

#### Root Cause Found:
**Environment Variable Inconsistency**:
- Integrations page: `const API_BASE = process.env.NEXT_PUBLIC_API_URL` ✅
- Dashboard page: `const API_BASE = process.env.NEXT_PUBLIC_API_URL` ✅  
- MappingDrawer component: `const API_BASE = process.env.NEXT_PUBLIC_API_BASE` ❌

#### Fix Applied:
**File**: `frontend/src/components/mapping-drawer.tsx`
```javascript
// BEFORE (broken):
const API_BASE = process.env.NEXT_PUBLIC_API_BASE || 'http://localhost:8000'

// AFTER (fixed):
const API_BASE = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000'
```

#### Console Error Evidence:
```
🚀 MappingDrawer: Fetching mapping data for platform: github
🚀 MappingDrawer: API Base: http://localhost:8000
❌ GET http://localhost:8000/integrations/mappings/platform/github net::ERR_CONNECTION_REFUSED
🚀 MappingDrawer: Error loading mapping data: TypeError: Failed to fetch
```

#### Expected Result After Deployment:
- MappingDrawer will use production API URL instead of localhost
- Dashboard mapping buttons will show data identical to integrations page
- All components will use consistent API endpoint configuration

#### Investigation Steps:
1. **Root Cause Analysis**: Both should use identical API calls and component
   - Dashboard uses: `<MappingDrawer isOpen={mappingDrawerOpen} platform={mappingDrawerPlatform} />`
   - Integrations uses: Built-in Sheet component with direct API calls
   - Same endpoints: `/integrations/mappings/platform/${platform}` and `/integrations/mappings/success-rate`

2. **Debugging Implementation**:
   - **Files Modified**:
     - `frontend/src/components/mapping-drawer.tsx` - Added 🚀 MappingDrawer debugging logs
     - `frontend/src/app/dashboard/page.tsx` - Added 🎯 Dashboard debugging logs
   - **Debug Tracking**:
     - Button click: `openMappingDrawer()` function calls
     - State updates: `mappingDrawerOpen` and `mappingDrawerPlatform` changes
     - Component lifecycle: `loadMappingData()` execution
     - API calls: Request/response status and data
     - State management: `setMappings()` and `setMappingStats()` calls

3. **Expected Debug Flow**:
   ```
   🎯 Dashboard: openMappingDrawer called with platform: github
   🎯 Dashboard: Set mappingDrawerPlatform to github and mappingDrawerOpen to true
   🚀 MappingDrawer: loadMappingData called - isOpen: true, platform: github
   🚀 MappingDrawer: Starting to load mapping data...
   🚀 MappingDrawer: Fetching mapping data for platform: github
   🚀 MappingDrawer: API Response statuses - mappings: 200, stats: 200
   🚀 MappingDrawer: Received X mappings and stats: {...}
   🚀 MappingDrawer: Successfully set mappings and stats state
   ```

4. **Potential Causes to Check**:
   - Component re-rendering issues
   - State timing problems (platform/isOpen synchronization)
   - API Base URL differences between pages
   - Authentication token consistency
   - useEffect dependency array issues
   - React 18 StrictMode double-mounting

#### Next Actions:
- [ ] Test dashboard mapping button with browser console open
- [ ] Compare console logs between working (integrations) and broken (dashboard) flows  
- [ ] Check if MappingDrawer useEffect is triggering properly
- [ ] Verify API_BASE environment variable consistency
- [ ] Test with React DevTools to inspect component state changes

#### Files Currently Under Investigation:
- `frontend/src/components/mapping-drawer.tsx` - Reusable component with debug logs
- `frontend/src/app/dashboard/page.tsx` - Dashboard implementation with debug logs
- `frontend/src/app/integrations/page.tsx` - Working reference implementation

### NEXT MAJOR ENHANCEMENT - GitHub-Only Burnout Analysis 🚧

#### Objective: Evaluate Burnout for Users with High GitHub Activity but Zero Incidents

**Problem Statement**: Current analysis only considers users who appear in incident data, missing potential burnout in developers who:
- Work long hours but aren't on-call rotation
- Have high code velocity but don't handle incidents
- Show burnout signals in development patterns before incidents occur
- Are junior developers not yet involved in incident response

#### Implementation Plan

##### Phase 1: Data Collection Enhancement
**Expand user collection to include GitHub-active developers**:

1. **Enhanced User Discovery**:
   - Current: Users discovered from Rootly incident data only
   - Enhanced: Users discovered from incident data + GitHub contributors + Slack active members
   - API calls: Combine Rootly users + GitHub org members + Slack workspace members

2. **GitHub Activity Thresholds**:
   - Minimum commits per analysis period (e.g., 5+ commits in 30 days)
   - Regular contribution patterns (not one-time contributors)
   - Activity across multiple repositories

3. **User Classification System**:
   ```python
   user_types = {
       "incident_responder": {"incidents": ">0", "github": "any", "slack": "any"},
       "pure_developer": {"incidents": "0", "github": ">threshold", "slack": "any"}, 
       "inactive": {"incidents": "0", "github": "<threshold", "slack": "low"},
       "communication_heavy": {"incidents": "any", "github": "low", "slack": ">threshold"}
   }
   ```

##### Phase 2: GitHub-Specific Burnout Indicators Based on Maslach Burnout Inventory
**Develop burnout detection mapped to validated psychological constructs**:

**Scientific Foundation**: Christina Maslach's research identifies three core dimensions:
1. **Emotional Exhaustion** - Feeling emotionally drained and depleted
2. **Depersonalization/Cynicism** - Detached, callous attitudes toward work
3. **Reduced Personal Accomplishment** - Feelings of ineffectiveness and lack of achievement

#### GitHub Data Mapping to MBI Dimensions:

**1. Emotional Exhaustion Indicators (40% weight)**:
   - **Temporal Overextension**:
     * Commits spread >12 hours/day consistently
     * Weekend commits >20% of total (normal: <10%)
     * After-hours commits (post-6PM, pre-8AM) trending upward
     * No "commit-free" days in 2+ week periods
   
   - **Intensity Without Recovery**:
     * Daily commit frequency 2+ std deviations above personal baseline
     * Commit timestamps showing <8 hours between last/first commits
     * Vacation periods with continued coding activity
     * Sprint periods without recovery weeks
   
   - **Methodology**: Compare against individual baseline + team norms
     ```python
     exhaustion_score = (
         temporal_overextension * 0.4 +
         intensity_without_recovery * 0.3 +
         baseline_deviation * 0.3
     )
     # Normalize to 0-100 scale where >70 = high risk
     ```

**2. Depersonalization/Cynicism Indicators (35% weight)**:
   - **Reduced Social Coding Behaviors**:
     * Code review participation drops >30% from baseline
     * Pull request descriptions become terse/minimal
     * Decreased helpful comments on others' PRs
     * Less participation in architectural discussions (measured via PR comments)
   
   - **Quality of Interaction Degradation**:
     * Commit messages become less descriptive (character count trend)
     * Increased use of generic messages ("fix", "update", "wip")
     * Reduced documentation contributions
     * Less mentoring activity (junior developer interactions)
   
   - **Defensive/Withdrawn Patterns**:
     * Smaller, more frequent commits (avoiding peer review)
     * Working in isolation (fewer collaborative commits)
     * Reduced cross-repository contributions
     * Less experimental/creative coding (measured by branch diversity)
   
   - **Methodology**: Baseline against historical collaboration patterns
     ```python
     cynicism_score = (
         social_coding_decline * 0.4 +
         interaction_quality_drop * 0.35 +
         withdrawal_patterns * 0.25
     )
     ```

**3. Reduced Personal Accomplishment Indicators (25% weight)**:
   - **Declining Code Quality Metrics**:
     * Increased bug-fix to feature-commit ratio
     * More reverts and rollbacks of own code  
     * Longer time to complete similar-sized features
     * Decreased complexity of problems tackled
   
   - **Productivity Paradox Signals**:
     * High commit volume with low meaningful progress
     * Increased "churn" (lines added then removed)
     * More "cleanup" commits vs substantial contributions
     * Decreased innovation (fewer new patterns/approaches)
   
   - **Achievement Pattern Changes**:
     * Longer PR cycles (feature delivery delays)
     * Reduced ownership of significant features
     * Less involvement in technical decision-making
     * Decreased cross-team collaboration impact
   
   - **Methodology**: Track accomplishment trends vs role expectations
     ```python
     accomplishment_score = 100 - (  # Inverted - lower = worse
         quality_decline * 0.4 +
         productivity_paradox * 0.35 +
         achievement_reduction * 0.25
     )
     ```

#### Composite GitHub Burnout Score:
```python
github_burnout_score = (
    emotional_exhaustion * 0.40 +
    cynicism_score * 0.35 +
    (100 - accomplishment_score) * 0.25  # Invert for consistent direction
)

# Risk Level Classification (aligned with clinical MBI ranges):
# 0-30: Low Risk (healthy patterns)
# 31-60: Moderate Risk (some concerning patterns)  
# 61-80: High Risk (multiple strong indicators)
# 81-100: Critical Risk (severe burnout indicators)
```

#### Critical Implementation Considerations:

**1. Individual Baseline Establishment**:
- Require minimum 3 months of historical data before scoring
- Account for role changes, project transitions, learning curves
- Seasonal adjustments (end-of-quarter pushes, vacation periods)
- Personal productivity patterns (some developers naturally work evenings)

**2. False Positive Prevention**:
- **Scenario**: Developer working "non-stop throughout the day"
- **Risk**: High exhaustion score from temporal patterns
- **Mitigation**: 
  * Cross-reference with productivity metrics (are they actually productive?)
  * Check for "flow state" patterns (consistent, sustainable output)
  * Validate against self-reported satisfaction/energy levels
  * Consider cultural/timezone factors (distributed teams)

**3. Statistical Rigor**:
```python
def calculate_burnout_with_confidence(user_data, baseline_data, team_norms):
    """
    Calculate burnout score with statistical confidence intervals
    """
    # Minimum data requirements
    if user_data.days_of_activity < 90:
        return {"score": None, "confidence": "insufficient_data"}
    
    # Calculate z-scores against multiple baselines
    personal_z_score = (current_metric - personal_baseline) / personal_std_dev
    team_z_score = (current_metric - team_average) / team_std_dev
    
    # Weight based on data quality and recency
    confidence_weight = min(1.0, user_data.days_of_activity / 180)
    
    # Composite score with confidence interval
    raw_score = calculate_raw_burnout_score(user_data)
    confidence_interval = calculate_confidence_interval(
        raw_score, sample_size, variance
    )
    
    return {
        "score": raw_score,
        "confidence_lower": confidence_interval.lower,
        "confidence_upper": confidence_interval.upper,
        "confidence_level": confidence_weight,
        "sample_size": user_data.days_of_activity
    }
```

**4. Ethical Considerations & Privacy**:
- Aggregate team trends vs individual surveillance
- Opt-in reporting for individuals
- Focus on systemic issues, not individual performance
- Clear communication about methodology and limitations
- Regular validation against actual burnout outcomes

**5. Validation Framework**:
```python
class BurnoutValidation:
    """Continuous validation against actual outcomes"""
    
    @staticmethod
    def validate_predictions():
        # Track prediction accuracy over time
        # Compare GitHub scores vs:
        # - Self-reported burnout surveys
        # - Sick leave patterns  
        # - Employee satisfaction scores
        # - Voluntary turnover
        # - Performance review outcomes
        
    @staticmethod  
    def adjust_weights():
        # Machine learning approach to optimize weights
        # Based on validation outcomes
        # Continuous model improvement
```

##### Phase 3: Scientifically Rigorous Scoring Implementation
**Multi-layered scoring system with statistical validation**:

**1. Data Quality Assessment**:
```python
class DataQualityCheck:
    @staticmethod
    def assess_data_sufficiency(user_data):
        """Ensure statistical validity before scoring"""
        quality_score = 0
        issues = []
        
        # Temporal coverage (minimum 90 days for baseline)
        if user_data.days_span >= 90:
            quality_score += 25
        else:
            issues.append(f"Insufficient history: {user_data.days_span} days")
            
        # Activity consistency (at least 50% of days with activity)
        activity_ratio = user_data.active_days / user_data.total_days
        if activity_ratio >= 0.3:
            quality_score += 25
        else:
            issues.append(f"Low activity consistency: {activity_ratio:.2%}")
            
        # Data diversity (commits, PRs, reviews - not just commits)
        data_types = sum([
            bool(user_data.commits),
            bool(user_data.pull_requests), 
            bool(user_data.code_reviews)
        ])
        quality_score += (data_types / 3) * 25
        
        # Recent activity (not just historical)
        days_since_last_activity = (datetime.now() - user_data.last_activity).days
        if days_since_last_activity <= 14:
            quality_score += 25
        else:
            issues.append(f"Stale data: {days_since_last_activity} days since activity")
            
        return {
            "quality_score": quality_score,
            "sufficient": quality_score >= 75,
            "issues": issues
        }
```

**2. Baseline Establishment (Multi-Modal)**:
```python
class BaselineCalculator:
    """Calculate multiple baseline types for robust comparison"""
    
    @staticmethod
    def calculate_personal_baseline(user_data, months_back=6):
        """Individual's own historical patterns"""
        # Rolling window approach - exclude last 30 days to avoid current burnout
        historical_data = user_data.exclude_recent(days=30)
        
        return {
            "commits_per_day": np.percentile(historical_data.daily_commits, 50),
            "work_hours_per_day": np.percentile(historical_data.daily_hours, 50),
            "review_participation": np.mean(historical_data.reviews_given),
            "code_quality_proxy": np.mean(historical_data.lines_per_commit),
            "variability": np.std(historical_data.daily_commits)
        }
    
    @staticmethod
    def calculate_cohort_baseline(team_data, user_role, user_seniority):
        """Baseline from similar developers (role + seniority)"""
        cohort = team_data.filter(role=user_role, seniority=user_seniority)
        
        # Use median (robust to outliers) rather than mean
        return {
            "commits_per_day": np.median([u.daily_commits for u in cohort]),
            "work_hours_per_day": np.median([u.daily_hours for u in cohort]),
            "review_participation": np.median([u.reviews_given for u in cohort]),
            "weekend_work_ratio": np.median([u.weekend_ratio for u in cohort])
        }
```

**3. Advanced Scoring with Confidence Intervals**:
```python
def calculate_github_burnout_comprehensive(user_data, baselines):
    """
    Maslach-aligned burnout scoring with statistical rigor
    Addresses the 'non-stop commits throughout day' scenario
    """
    
    # 1. EMOTIONAL EXHAUSTION (40% weight)
    temporal_score = calculate_temporal_exhaustion(user_data, baselines)
    intensity_score = calculate_intensity_exhaustion(user_data, baselines)
    
    # Key insight: Distinguish between 'flow state' and 'frantic activity'
    flow_state_indicator = detect_flow_vs_frantic(user_data)
    if flow_state_indicator.is_sustainable_flow:
        # Healthy high-productivity - reduce exhaustion penalty
        temporal_score *= 0.7
        intensity_score *= 0.8
    
    exhaustion_raw = (temporal_score * 0.6 + intensity_score * 0.4)
    
    # 2. CYNICISM/DEPERSONALIZATION (35% weight) 
    social_decline = calculate_social_coding_decline(user_data, baselines)
    interaction_quality = calculate_interaction_degradation(user_data, baselines)
    
    cynicism_raw = (social_decline * 0.6 + interaction_quality * 0.4)
    
    # 3. REDUCED ACCOMPLISHMENT (25% weight)
    quality_decline = calculate_quality_trends(user_data, baselines)
    productivity_paradox = calculate_productivity_paradox(user_data, baselines)
    
    accomplishment_raw = 100 - (quality_decline * 0.6 + productivity_paradox * 0.4)
    
    # Composite score
    raw_score = (
        exhaustion_raw * 0.40 +
        cynicism_raw * 0.35 +
        (100 - accomplishment_raw) * 0.25
    )
    
    # Calculate confidence interval
    confidence = calculate_confidence_interval(user_data, raw_score)
    
    return BurnoutAssessment(
        score=raw_score,
        confidence_lower=confidence.lower,
        confidence_upper=confidence.upper,
        reliability=confidence.reliability,
        components={
            "exhaustion": exhaustion_raw,
            "cynicism": cynicism_raw, 
            "accomplishment": accomplishment_raw
        },
        risk_factors=identify_primary_risk_factors(user_data),
        recommendations=generate_recommendations(raw_score, user_data)
    )

def detect_flow_vs_frantic(user_data):
    """
    Critical function: Distinguish sustainable high-productivity from burnout
    
    Flow State Indicators:
    - Consistent output quality maintained
    - Breaks between intensive periods
    - Productive output per hour remains high
    - Code review engagement remains healthy
    
    Frantic Activity Indicators:
    - Output quality declining despite high volume
    - No recovery periods
    - Inefficient commits (high churn, many micro-commits)
    - Social engagement dropping
    """
    
    # Quality maintenance during high activity
    quality_consistency = np.corrcoef(
        user_data.daily_commit_count,
        user_data.daily_quality_score
    )[0,1]
    
    # Efficiency metrics
    commits_per_productive_hour = user_data.commits / user_data.focused_hours
    efficiency_trend = np.polyfit(range(len(commits_per_productive_hour)), 
                                 commits_per_productive_hour, 1)[0]
    
    # Recovery pattern detection
    has_recovery_periods = detect_rest_periods(user_data.daily_activity)
    
    # Social engagement maintenance
    social_engagement_trend = calculate_social_trend(user_data.review_activity)
    
    is_flow = (
        quality_consistency > 0.1 and  # Quality maintained during high activity
        efficiency_trend >= -0.1 and   # Efficiency not declining rapidly
        has_recovery_periods and       # Taking breaks
        social_engagement_trend >= -0.2 # Social coding not collapsing
    )
    
    return FlowStateAssessment(
        is_sustainable_flow=is_flow,
        quality_maintenance=quality_consistency,
        efficiency_trend=efficiency_trend,
        recovery_detected=has_recovery_periods,
        social_health=social_engagement_trend
    )
```

**4. Risk Classification (Clinical MBI Alignment)**:
```python
def classify_burnout_risk(burnout_assessment):
    """Map to clinically validated MBI risk levels"""
    
    score = burnout_assessment.score
    confidence = burnout_assessment.reliability
    
    # Adjust risk classification based on confidence
    if confidence < 0.7:
        return RiskClassification(
            level="INSUFFICIENT_DATA",
            message="Need more data for reliable assessment",
            recommendation="Continue monitoring for 30+ days"
        )
    
    # Clinical MBI thresholds adapted for GitHub data
    if score >= 81:
        return RiskClassification(
            level="CRITICAL_RISK",
            message="Severe burnout indicators across multiple dimensions",
            recommendation="Immediate intervention recommended",
            confidence_interval=(burnout_assessment.confidence_lower, 
                                burnout_assessment.confidence_upper)
        )
    elif score >= 61:
        return RiskClassification(
            level="HIGH_RISK", 
            message="Multiple strong burnout indicators",
            recommendation="Schedule check-in within 1 week"
        )
    elif score >= 31:
        return RiskClassification(
            level="MODERATE_RISK",
            message="Some concerning patterns emerging", 
            recommendation="Monitor closely, consider workload review"
        )
    else:
        return RiskClassification(
            level="LOW_RISK",
            message="Healthy development patterns",
            recommendation="Continue current practices"
        )
```

##### Phase 4: Dashboard Integration
**Display GitHub-only burnout analysis**:

1. **Enhanced Team Overview**:
   - Show all team members (not just incident-involved)
   - GitHub activity indicators for each member
   - Burnout risk levels for pure developers

2. **GitHub-Specific Insights**:
   - Developer velocity trends over time
   - Code quality metrics progression
   - Work-life balance scoring
   - Collaboration health indicators

3. **Actionable Recommendations**:
   - Workload redistribution suggestions
   - Code review process improvements
   - Work-life balance interventions
   - Mentoring/support recommendations

##### Phase 5: Implementation Steps

1. **Backend Changes**:
   - `backend/app/services/github_user_discovery.py` - New user discovery logic
   - `backend/app/services/github_burnout_analyzer.py` - GitHub-specific analysis
   - `backend/app/models/user_classification.py` - User type tracking
   - Enhanced existing analyzers to handle mixed data sources

2. **API Endpoints**:
   - `/analyses/github-only` - Pure GitHub burnout analysis
   - `/team/github-contributors` - Discover GitHub-active team members
   - `/users/{id}/github-patterns` - Individual developer pattern analysis

3. **Frontend Updates**:
   - Team overview showing all members (not just incident-involved)
   - GitHub activity cards for pure developers
   - Burnout risk indicators for code-focused work
   - Filtering options: "All Members", "Incident Responders", "Developers Only"

4. **Database Schema**:
   ```sql
   ALTER TABLE users ADD COLUMN user_type VARCHAR(50);
   ALTER TABLE users ADD COLUMN github_activity_score FLOAT;
   ALTER TABLE users ADD COLUMN last_github_activity TIMESTAMP;
   
   CREATE TABLE github_burnout_indicators (
       id SERIAL PRIMARY KEY,
       user_id INTEGER REFERENCES users(id),
       analysis_id INTEGER REFERENCES analyses(id),
       velocity_score FLOAT,
       quality_score FLOAT,
       work_life_score FLOAT,
       collaboration_score FLOAT,
       created_at TIMESTAMP DEFAULT NOW()
   );
   ```

##### Benefits Expected:
1. **Proactive Burnout Detection**: Catch burnout before it leads to incidents
2. **Complete Team Coverage**: Include all active team members, not just on-call
3. **Developer-Specific Insights**: Code-focused burnout indicators
4. **Earlier Intervention**: Address work-life balance issues before escalation
5. **Role-Appropriate Analysis**: Different burnout patterns for different roles

##### Success Metrics:
- % of team members analyzed (target: 90%+ of active contributors)
- Early burnout detection rate (GitHub signals before incident involvement)
- Developer satisfaction with work-life balance recommendations
- Reduction in developer churn/turnover
- Improvement in code quality metrics over time

### Outstanding Issues:
1. Slack channel access errors (bot not in channels) - Low priority
2. Invalid Anthropic API key for AI narratives - User configuration issue

### Recently Completed:
1. ✅ **Health Trends Chart Logic** - Fixed to show daily incident data from current analysis period
2. ✅ **Data Consistency Issue** - Fixed major inconsistency where dashboard showed different incident counts across components
3. ✅ **GitHub Mapping Button Navigation Fix** - Changed dashboard GitHub button from navigation to MappingDrawer
4. ✅ **Integrations Page Debug Enhancement** - Added comprehensive logging for endless loader troubleshooting

#### 6. Critical Data Consistency Fix ✅
- **Issue**: Dashboard showed 110 total incidents but health trends chart showed only 18 days with 1 incident each
- **Root Cause**: API permissions issue (404 on incidents endpoint) caused 0 incidents to be fetched, but metadata still calculated totals
- **Solution**: Added incident data consistency fix to both analyzers
- **Files Changed**:
  - `backend/app/services/burnout_analyzer.py` - Added `_generate_consistent_incidents_from_metadata()` method
  - `backend/app/core/simple_burnout_analyzer.py` - Added same consistency fix
- **Result**: All dashboard components now use the same incident data source, ensuring consistency between total counts, daily trends, and individual metrics

### COMPREHENSIVE DATA COLLECTION & STORAGE PLAN

#### Objective: Maximize Data Granularity for Superior Analysis Results

**Core Principle**: "Store everything we can access - more data = better insights"

#### 1. Enhanced API Data Collection

**Rootly Incidents API - Collect ALL Available Fields**:
```python
# Current: Basic incident data
# Enhanced: Comprehensive incident metadata
incident_data = {
    # Core fields (already collected)
    "id": incident.id,
    "title": incident.title,
    "status": incident.status,
    "severity": incident.severity,
    "created_at": incident.created_at,
    "resolved_at": incident.resolved_at,
    
    # NEW: Enhanced metadata
    "description": incident.description,
    "summary": incident.summary,
    "impact": incident.impact,
    "priority": incident.priority,
    "category": incident.category,
    "subcategory": incident.subcategory,
    "environment": incident.environment,
    "services_affected": incident.services,
    "root_cause": incident.root_cause,
    "resolution_notes": incident.resolution_notes,
    
    # NEW: Timeline and response metrics
    "first_response_time_minutes": calculate_first_response_time(incident),
    "time_to_acknowledge_minutes": calculate_ack_time(incident),
    "time_to_resolve_minutes": calculate_resolve_time(incident),
    "escalation_count": count_escalations(incident),
    "number_of_updates": len(incident.updates),
    
    # NEW: Team involvement
    "assignees": [user.id for user in incident.assignees],
    "responders": [user.id for user in incident.responders],
    "followers": [user.id for user in incident.followers],
    "incident_commander": incident.commander.id if incident.commander else None,
    "escalated_to": incident.escalated_to if hasattr(incident, 'escalated_to') else None,
    
    # NEW: Communication metrics
    "total_comments": len(incident.comments),
    "external_communications": count_external_comms(incident),
    "stakeholder_updates": count_stakeholder_updates(incident),
    "notification_channels": incident.notification_channels,
    
    # NEW: Business impact
    "customer_impact_level": incident.customer_impact,
    "financial_impact": incident.financial_impact,
    "affected_user_count": incident.affected_users,
    "business_services_impacted": incident.business_services,
    
    # NEW: Detection and response patterns
    "detection_method": incident.detection_method,
    "automated_detection": incident.automated_detection,
    "alert_source": incident.alert_source,
    "monitoring_tools": incident.monitoring_tools,
    
    # NEW: Post-incident data
    "post_mortem_completed": bool(incident.post_mortem),
    "action_items_count": count_action_items(incident),
    "lessons_learned": incident.lessons_learned,
    "follow_up_required": incident.follow_up_required
}
```

**GitHub Activity API - Comprehensive Code Metrics**:
```python
github_metrics = {
    # Current: Basic commit/PR data
    # Enhanced: Development pattern analysis
    "commits": {
        "total_count": len(commits),
        "commits_by_hour": group_commits_by_hour(commits),
        "commits_by_day_of_week": group_commits_by_weekday(commits),
        "weekend_commits": count_weekend_commits(commits),
        "after_hours_commits": count_after_hours_commits(commits),
        "commit_message_length_avg": avg_commit_message_length(commits),
        "commit_size_lines_avg": avg_lines_changed(commits),
        "force_pushes": count_force_pushes(commits),
        "merge_commits": count_merge_commits(commits),
        "revert_commits": count_revert_commits(commits)
    },
    "pull_requests": {
        "total_prs": len(pull_requests),
        "draft_prs": count_draft_prs(pull_requests),
        "pr_review_time_avg": avg_pr_review_time(pull_requests),
        "self_approved_prs": count_self_approved(pull_requests),
        "large_prs": count_large_prs(pull_requests),  # >500 lines
        "pr_comments_avg": avg_pr_comments(pull_requests),
        "pr_iterations_avg": avg_pr_iterations(pull_requests),
        "hotfix_prs": count_hotfix_prs(pull_requests)
    },
    "code_review_patterns": {
        "reviews_given": count_reviews_given(user),
        "reviews_received": count_reviews_received(user),
        "review_response_time_avg": avg_review_response_time(user),
        "constructive_feedback_ratio": calculate_feedback_ratio(user),
        "approval_rate": calculate_approval_rate(user)
    },
    "repository_activity": {
        "repos_contributed_to": count_unique_repos(user),
        "primary_languages": get_primary_languages(user),
        "issue_creation": count_issues_created(user),
        "issue_comments": count_issue_comments(user),
        "wiki_edits": count_wiki_edits(user),
        "release_participation": count_release_participation(user)
    }
}
```

**Slack Communications API - Deep Communication Analysis**:
```python
slack_metrics = {
    # Current: Basic message counts
    # Enhanced: Communication pattern analysis
    "messaging_patterns": {
        "total_messages": len(messages),
        "messages_by_hour": group_messages_by_hour(messages),
        "weekend_messages": count_weekend_messages(messages),
        "after_hours_messages": count_after_hours_messages(messages),
        "thread_participation": count_thread_messages(messages),
        "broadcast_messages": count_broadcast_messages(messages),
        "direct_messages_sent": count_dm_sent(messages),
        "direct_messages_received": count_dm_received(messages)
    },
    "communication_quality": {
        "message_length_avg": avg_message_length(messages),
        "emoji_usage": count_emoji_usage(messages),
        "reaction_patterns": analyze_reaction_patterns(messages),
        "urgency_indicators": count_urgency_keywords(messages),
        "question_asking_rate": count_questions_asked(messages),
        "helping_behavior": count_help_provided(messages)
    },
    "collaboration_metrics": {
        "channels_active_in": count_active_channels(user),
        "channel_creation": count_channels_created(user),
        "mentions_given": count_mentions_given(user),
        "mentions_received": count_mentions_received(user),
        "file_shares": count_file_shares(user),
        "link_shares": count_link_shares(user)
    },
    "incident_communication": {
        "incident_channel_messages": count_incident_messages(user),
        "status_updates_provided": count_status_updates(user), 
        "escalation_messages": count_escalation_messages(user),
        "resolution_confirmations": count_resolution_messages(user),
        "post_incident_discussion": count_post_incident_messages(user)
    },
    "sentiment_analysis": {
        "message_sentiment_avg": calculate_avg_sentiment(messages),
        "sentiment_trend": calculate_sentiment_trend(messages),
        "stress_indicators": count_stress_keywords(messages),
        "frustration_indicators": count_frustration_keywords(messages),
        "positive_language_ratio": calculate_positive_ratio(messages)
    }
}
```

#### 2. Raw API Response Storage

**Complete API Response Archival**:
```python
# Store complete API responses for debugging and future analysis
api_response_archive = {
    "rootly_incidents_raw": {
        "timestamp": datetime.utcnow(),
        "endpoint": "/incidents",
        "query_params": {"start_date": "...", "end_date": "..."},
        "response_status": 200,
        "raw_response": complete_api_response,  # Full JSON response
        "response_headers": dict(response.headers),
        "processing_notes": "Any issues or observations during processing"
    },
    "github_activity_raw": {
        "timestamp": datetime.utcnow(),
        "endpoints_called": ["/user/commits", "/user/pulls", "/user/events"],
        "raw_responses": [response1, response2, response3],
        "rate_limit_info": github_rate_limit_status,
        "processing_notes": "API rate limit status, any pagination issues"
    },
    "slack_messages_raw": {
        "timestamp": datetime.utcnow(), 
        "channels_queried": ["#incidents", "#engineering", "#alerts"],
        "raw_responses": channel_message_responses,
        "message_count_total": total_messages_retrieved,
        "processing_notes": "Channel access permissions, message filtering applied"
    }
}
```

#### 3. Enhanced Calculated Metrics

**Advanced Burnout Indicators**:
```python
advanced_burnout_metrics = {
    # Workload intensity patterns
    "workload_patterns": {
        "incident_clustering": {
            "incidents_per_day_avg": avg_incidents_per_day,
            "max_incidents_single_day": max_daily_incidents,
            "incident_free_days": count_incident_free_days,
            "consecutive_incident_days": longest_incident_streak,
            "weekend_incident_ratio": weekend_incidents / total_incidents,
            "holiday_incidents": count_holiday_incidents,
            "time_between_incidents_avg": avg_time_between_incidents
        },
        "response_patterns": {
            "fastest_response_time": min_response_time,
            "slowest_response_time": max_response_time,
            "response_time_consistency": response_time_std_dev,
            "late_night_responses": count_responses_after_midnight,
            "immediate_responses": count_responses_under_5min,
            "response_degradation": calculate_response_time_trend
        }
    },
    
    # Stress progression indicators
    "stress_progression": {
        "communication_changes": {
            "message_length_trend": calculate_message_length_trend,
            "response_delay_trend": calculate_response_delay_trend,
            "emoji_usage_change": calculate_emoji_trend,
            "formality_increase": calculate_formality_trend,
            "question_asking_decrease": calculate_question_trend
        },
        "work_pattern_changes": {
            "working_hours_expansion": calculate_hours_expansion,
            "weekend_work_increase": calculate_weekend_increase,
            "break_frequency_decrease": calculate_break_decrease,
            "multitasking_increase": calculate_multitask_increase
        }
    },
    
    # Recovery and resilience indicators
    "recovery_patterns": {
        "post_incident_activity": {
            "follow_up_time": time_to_follow_up_actions,
            "documentation_completeness": assess_documentation_quality,
            "knowledge_sharing": count_knowledge_sharing_activities,
            "process_improvement_suggestions": count_improvement_suggestions
        },
        "learning_indicators": {
            "similar_incident_handling_improvement": measure_handling_improvement,
            "proactive_monitoring_setup": count_monitoring_improvements,
            "automation_contributions": count_automation_efforts,
            "mentoring_activity": count_mentoring_activities
        }
    }
}
```

#### 4. Historical Trend Analysis

**Long-term Pattern Storage**:
```python
historical_analysis = {
    "weekly_trends": {
        "week_1": {"incidents": 5, "avg_response": 45, "burnout_score": 6.2},
        "week_2": {"incidents": 8, "avg_response": 52, "burnout_score": 6.8},
        # ... store weekly snapshots for trend analysis
    },
    "monthly_comparisons": {
        "month_over_month_changes": {
            "incident_volume_change": "+15%",
            "response_time_change": "+12%",
            "team_health_change": "-8%",
            "automation_adoption": "+25%"
        }
    },
    "seasonal_patterns": {
        "peak_incident_months": ["March", "September"],  # Release seasons
        "lowest_activity_periods": ["December", "August"],  # Holiday seasons
        "on_call_rotation_effectiveness": analyze_rotation_success
    }
}
```

#### 5. Cross-System Correlation Analysis

**Multi-Source Data Correlation**:
```python
correlation_analysis = {
    "incident_to_code_correlation": {
        "incidents_following_large_deployments": count_post_deploy_incidents,
        "code_complexity_vs_incidents": correlate_complexity_incidents,
        "review_thoroughness_vs_stability": correlate_reviews_stability,
        "hotfix_frequency_vs_burnout": correlate_hotfix_burnout
    },
    "communication_to_performance_correlation": {
        "response_time_vs_message_sentiment": correlate_time_sentiment,
        "collaboration_level_vs_resolution_speed": correlate_collab_speed,
        "knowledge_sharing_vs_incident_prevention": correlate_knowledge_prevention
    },
    "temporal_correlations": {
        "time_of_day_vs_incident_severity": analyze_time_severity,
        "day_of_week_vs_response_quality": analyze_day_quality,
        "season_vs_team_health": analyze_seasonal_health
    }
}
```

#### 6. Implementation Plan

**Database Schema Enhancements**:
```sql
-- New tables for granular data storage
CREATE TABLE incident_details (
    id SERIAL PRIMARY KEY,
    incident_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_api_response JSONB,
    enhanced_metrics JSONB,
    timeline_data JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE github_activity_details (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_api_responses JSONB,
    calculated_metrics JSONB,
    pattern_analysis JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE slack_communication_details (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    analysis_id INTEGER REFERENCES analyses(id),
    raw_messages JSONB,
    sentiment_analysis JSONB,
    communication_patterns JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE analysis_correlations (
    id SERIAL PRIMARY KEY,
    analysis_id INTEGER REFERENCES analyses(id),
    correlation_type VARCHAR(100),
    correlation_data JSONB,
    significance_score FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**File Structure for Enhanced Data Collection**:
```
backend/app/services/
├── enhanced_data_collector.py      # Orchestrates all data collection
├── rootly_enhanced_collector.py    # Comprehensive Rootly data
├── github_enhanced_collector.py    # Detailed GitHub analysis
├── slack_enhanced_collector.py     # Deep Slack communication analysis
├── correlation_analyzer.py         # Cross-system pattern analysis
├── pattern_detector.py            # Advanced pattern recognition
└── data_archiver.py               # Raw response storage and retrieval
```

**Configuration for Maximum Data Collection**:
```python
# config/data_collection.py
DATA_COLLECTION_CONFIG = {
    "rootly": {
        "collect_all_incident_fields": True,
        "include_comments": True,
        "include_timeline": True,
        "include_post_mortems": True,
        "archive_raw_responses": True
    },
    "github": {
        "analyze_commit_patterns": True,
        "include_code_review_metrics": True,
        "analyze_repository_activity": True,
        "include_issue_participation": True,
        "archive_raw_responses": True
    },
    "slack": {
        "perform_sentiment_analysis": True,
        "analyze_communication_patterns": True,
        "include_emoji_analysis": True,
        "track_collaboration_metrics": True,
        "archive_raw_messages": True
    },
    "analysis": {
        "calculate_correlations": True,
        "detect_patterns": True,
        "generate_predictions": True,
        "store_intermediate_calculations": True
    }
}
```

#### Expected Benefits:
1. **Better Burnout Prediction**: More data points = earlier warning signs
2. **Deeper Root Cause Analysis**: Full context for why burnout occurs
3. **Personalized Recommendations**: Individual patterns drive specific advice
4. **Trend Analysis**: Historical data reveals long-term patterns
5. **Preventive Insights**: Predict issues before they become critical
6. **Debugging Capability**: Raw data helps troubleshoot analysis issues
7. **Research Opportunities**: Rich dataset enables burnout research

#### Data Retention Policy:
- **Raw API Responses**: 1 year (for debugging and reprocessing)
- **Calculated Metrics**: Permanent (for trend analysis)
- **Personal Communication Data**: 6 months (privacy compliance)
- **Aggregated Team Metrics**: Permanent (organizational insights)